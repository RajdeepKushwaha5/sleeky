{
  "lastUpdated": "2025-11-02T11:01:14.337Z",
  "posts": [
    {
      "id": "https://medium.com/p/cc7c040e3514",
      "title": "System Design Made Simple: A Complete Beginnerâ€™s Guide",
      "description": "",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*p2rGe_CoFyaoEgZF\" /></figure><h3>I. FOUNDATIONAL CONCEPTS</h3><p>1.1 Why Study System Design?<br>1.2 What is a Server?<br>1.3 Latency and Throughput<br>1.4 Scaling and ItsÂ Types</p><ul><li>Vertical Scaling</li><li>Horizontal Scaling<br>1.5 Auto Scaling<br>1.6 Back-of-the-Envelope Estimation</li></ul><h3>II. CORE PRINCIPLES &amp;Â THEOREMS</h3><p>2.1 CAP Theorem<br>2.2 Consistency DeepÂ Dive</p><ul><li>Strong Consistency</li><li>Eventual Consistency</li><li>When to Use Each Type<br>2.3 Distributed Systems Fundamentals</li></ul><h3>III. DATABASE SCALING STRATEGIES</h3><p>3.1 Database Scaling Overview<br>3.2 Indexing<br>3.3 Partitioning<br>3.4 Master-Slave Architecture<br>3.5 Multi-master Setup<br>3.6 DatabaseÂ Sharding</p><ul><li>Sharding Strategies</li><li>Disadvantages of Sharding<br>3.7 Database Scaling Summary<br>3.8 SQL vs NoSQL Databases</li><li>SQL Databases</li><li>NoSQL Databases</li><li>When to Use WhichÂ Database</li></ul><h3>IV. ARCHITECTURE PATTERNS</h3><p>4.1 Microservices Architecture</p><ul><li>Monolith vs Microservices</li><li>Why Use Microservices?</li><li>API Gateway Pattern<br>4.2 Event-Driven Architecture (EDA)</li><li>Introduction toÂ EDA</li><li>Simple Event Notification</li><li>Event-Carried State Transfer<br>4.3 Load Balancer DeepÂ Dive</li><li>Why Load Balancers?</li><li>Load Balancer Algorithms<br>4.4 ProxyÂ Systems</li><li>Forward Proxy</li><li>Reverse Proxy</li><li>Building Your Own ReverseÂ Proxy</li></ul><h3>V. PERFORMANCE OPTIMIZATION</h3><p>5.1 Caching Fundamentals</p><ul><li>Caching Introduction</li><li>Benefits ofÂ Caching</li><li>Types of Caches<br>5.2 Redis DeepÂ Dive</li><li>Redis DataÂ Types</li><li>Redis Implementation Examples<br>5.3 Content Delivery NetworkÂ (CDN)</li><li>CDN Introduction</li><li>How CDNÂ Works</li><li>Key CDNÂ Concepts</li></ul><h3>VI. STORAGE SOLUTIONS</h3><p>6.1 BlobÂ Storage</p><ul><li>What is BlobÂ Storage?</li><li>AWS S3 Overview<br>6.2 Data Redundancy andÂ Recovery</li><li>Why Data Redundancy?</li><li>Backup Strategies</li><li>Continuous Redundancy</li></ul><h3>VII. MESSAGING &amp; COMMUNICATION</h3><p>7.1 MessageÂ Brokers</p><ul><li>Synchronous vs Asynchronous</li><li>Why Use Message Brokers?<br>7.2 Message Queues vs Message Streams<br>7.3 Apache Kafka DeepÂ Dive</li><li>Kafka Internals</li><li>When to Use Kafka<br>7.4 Real-time Pub/Sub</li></ul><h3>VIII. ADVANCED DISTRIBUTED CONCEPTS</h3><p>8.1 Consistent Hashing<br>8.2 Auto-Recoverable Systems</p><ul><li>Leader Election</li><li>Orchestrator Patterns<br>8.3 Big DataÂ Tools</li><li>Apache SparkÂ Overview</li><li>When to Use Distributed Processing</li></ul><h3>IX. PRACTICAL IMPLEMENTATION</h3><p>9.1 Hands-On Exercises</p><ul><li>Deployment Exercises</li><li>Configuration Exercises</li><li>Coding Challenges<br>9.2 Quick Learning Checks<br>9.3 Node.js Implementation Examples</li><li>Redis CachingÂ Code</li><li>Reverse ProxyÂ Code</li></ul><h3>X. PROBLEM-SOLVING FRAMEWORK</h3><p>10.1 How to Solve Any System Design Problem<br>10.2 Step-by-Step Approach<br>10.3 Common Patterns and Anti-patterns</p><h3>XI. SUMMARY &amp; NEXTÂ STEPS</h3><p>11.1 Key Takeaways<br>11.2 Learning Path Recommendations<br>11.3 Additional Resources</p><h3>I. FOUNDATIONAL CONCEPTS</h3><h3>1.1 Why Study SystemÂ Design?</h3><p>Have you ever posted a photo on Instagram and had it appear for your friends across the globe in a second? Or started a movie on Netflix without it ever buffering? The magic behind these seamless experiences is SystemÂ Design.</p><p>Think of system design as the art of creating a blueprint for a software application. But instead of a simple drawing, itâ€™s a plan for a powerful, resilient, and scalable machine that can serve millions of users without breaking aÂ sweat.</p><p>If youâ€™ve built personal projects with a backend and a database, youâ€™re already on the right path. This guide will show you how to evolve that simple setup into the kind of robust architecture used by tech giants. Letâ€™s build your foundation!</p><h3>1.2 What is a Server? The Computer That NeverÂ Sleeps</h3><p>You might already know this, but letâ€™s make sure weâ€™re all on the same page! A server is essentially a powerful computer thatâ€™s always on and connected to the internet, running your application code.</p><p>When you run your app on http://localhost:8080, &quot;localhost&quot; is your own laptop. For the real world, we need a server with a publicÂ address.</p><ul><li><strong>Domain Names &amp; IPs: </strong>You type google.com into your browser, but computers talk using numbers called IP Addresses (like 142.251.42.206). A DNS (Domain Name System) acts like the internet&#39;s phonebook, translating google.com into that IPÂ address.</li><li><strong>Ports:</strong> A server runs many applications. Ports are like apartment numbers for those applications, ensuring a request for a website goes to the web server software and not the emailÂ server.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*FQ1WqrX2-2oFxLdp\" /></figure><blockquote><strong><em>ğŸ’¡ Try This:</em></strong><em> The next time you visit a website, try running </em><em>nslookup [website-url] in your command prompt (or Terminal). You&#39;ll see the actual IP address your computer is talkingÂ to!</em></blockquote><h3>1.3 Latency and Throughput: The Speed and theÂ Volume</h3><p>These two terms are the heartbeat of any systemâ€™s performance.</p><ul><li><strong>Latency:</strong> The time taken for a single request to go from the client to the server and back. Itâ€™s measured in milliseconds (ms). Low latency is fast; high latency isÂ slow.</li><li><strong>Throughput:</strong> The number of requests your system can handle per second. Itâ€™s measured in requests per second (RPS). High throughput means handling more users simultaneously.</li></ul><p>A SimpleÂ Analogy:</p><ul><li><strong>Latency:</strong> The time it takes for a single car to travel from Point A to Point B (e.g., 10 minutes).</li><li><strong>Throughput:</strong> The number of cars that can travel on a highway in one hour (e.g., 6,000Â cars).</li></ul><p><strong>Our Goal:</strong> To build systems with low latency (fast for the user) and high throughput (can handle manyÂ users).</p><h3>1.4 Scaling and Its Types: Preparing for aÂ Crowd</h3><p>When a popular website crashes due to traffic, itâ€™s often because it couldnâ€™t scale. Scaling means enhancing your systemâ€™s capacity to handle increased load.</p><p>Think of your phone: a cheap phone with less RAM slows down when you open too many apps. Your server behaves the same way under heavy traffic. Scaling is the solution.</p><h3>Vertical Scaling (ScalingÂ Up)</h3><p>What it is: Adding more power (CPU, RAM, Storage) to your existing server.<br>When to use it: Often used for databases (like SQL) where itâ€™s simpler than distributing data.<br>The Problem: You canâ€™t upgrade a single server forever. Thereâ€™s a physicalÂ limit.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/860/0*qtqUE9Mc1gPmvTN1\" /></figure><h3>Horizontal Scaling (ScalingÂ Out)</h3><p><strong>What it is:</strong> Adding more servers to your pool to share the load.<br>The Challenge: Clients canâ€™t be expected to know about all these different servers.</p><p><strong>The Solution:</strong> The Load Balancer. This is a traffic cop for your servers. All client requests go to the load balancer, which intelligently routes each one to a healthy, less-busy server.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/860/0*2r7ZjNlVm4W6QrY3\" /></figure><blockquote><em>ğŸ’¡ </em><strong><em>Try This:</em></strong><em> Imagine youâ€™re launching a new game. You start with one server, but on launch day, traffic explodes. Would you choose Vertical or Horizontal scaling?Â Why?</em></blockquote><h3>1.5 Auto Scaling: The Smart Assistant</h3><p>Running 100 servers all the time for traffic that only needs 10 is wasteful. Auto Scaling is the solution: it automatically adds or removes servers based on real-time traffic (e.g., when CPU usage crossesÂ 70%).</p><p>This gives you both performance during peaks and cost savings duringÂ lulls.</p><h3>1.6 Back-of-the-Envelope Estimation: The Art of SmartÂ Guessing</h3><p>Before building, we estimate the resources weâ€™ll need. In interviews, spend ~5 minutes on this. We use approximations to make mathÂ easy.</p><p>Handy Table for Estimation:</p><p>Power of 2Approx. ValuePower of 10Full NameShort Name2Â¹â°Â¹ Thousand10Â³KilobyteKB2Â²â°Â¹ Million10â¶MegabyteMB2Â³â°Â¹ Billion10â¹GigabyteGB2â´â°Â¹ Trillion10Â¹Â²TerabyteTB</p><p>Example: Estimating for a Twitter-like App</p><ul><li>Load Estimation:</li><li>Assume 100 million Daily Active UsersÂ (DAU).</li><li>Each user posts 10 tweets/day â†’ 1 billion writes/day.</li><li>Each user reads 1000 tweets/day â†’ 100 billion reads/day.</li><li>Storage Estimation:</li><li>Assume a tweet is 500 bytes and 10% have a 2MBÂ photo.</li><li>Daily Storage = (1B tweets * 500 bytes) + (100M photos * 2MB) â‰ˆ 1 Petabyte (PB)/day.</li><li>Resource Estimation:</li><li>Assume 10,000 requests/second, each taking 10ms of CPUÂ time.</li><li>Total CPU time needed = 100,000 ms perÂ second.</li><li>If one CPU core handles 1000 ms/sec, you need 100Â cores.</li><li>With 4-core servers â†’ 25 serversÂ needed.</li></ul><h3>II. CORE PRINCIPLES &amp;Â THEOREMS</h3><h3>2.1 CAP Theorem: The Impossible Choice</h3><p>Think of CAP Theorem as a â€œPick Twoâ€ menu at a restaurant:</p><p>You have 3 delicious items, but you can only chooseÂ 2:</p><ul><li>C = Consistency (Everyone sees the sameÂ data)</li><li>A = Availability (System always responds)</li><li>P = Partition Tolerance (Works even when networksÂ fail)</li></ul><p>The Reality: Network failures WILL happen, so you MUST choose P. Now youâ€™re leftÂ with:</p><h3>CP (Consistency + Partition Tolerance)</h3><ul><li>â€œIâ€™d rather be silent thanÂ wrongâ€</li><li>During network problems, the system stops responding to ensure no one sees inconsistent data</li><li>Example: Banking appsâ€Šâ€”â€Šif thereâ€™s a network issue, itâ€™s better to show â€œSystem Downâ€ than show wrong accountÂ balances</li></ul><h3>AP (Availability + Partition Tolerance)</h3><ul><li>â€œIâ€™d rather be fast than perfectly accurateâ€</li><li>During network problems, the system keeps responding but might show slightly oldÂ data</li><li>Example: Social mediaâ€Šâ€”â€Šif likes are delayed by a few seconds, itâ€™s acceptable</li></ul><p>You CANNOT have all three! Itâ€™s like trying to be in two places at onceâ€Šâ€”â€Šphysically impossible.</p><h3>2.2 Consistency Deep Dive: The Truth AboutÂ Truth</h3><h3>Strong Consistency: The Perfectionist</h3><p><strong>Analogy:</strong> A synchronized swimmingÂ team</p><ul><li>Every move is perfectly coordinated</li><li>Everyone sees exactly the same thing at exactly the sameÂ time</li><li>If one person is out of sync, they stop until everyone catchesÂ up</li></ul><p><strong>How itÂ works:</strong></p><ul><li>When you update data, the system WAITS until all copies are updated before saying â€œsuccessâ€</li><li>Every read after a write is guaranteed to show the latestÂ data</li></ul><p><strong>Real-world examples:</strong></p><ul><li>ğŸ¦ Banking: Your account balance must beÂ accurate</li><li>ğŸ’³ Payment systems: Canâ€™t double-charge customers</li><li>ğŸ“ˆ Stock trading: Prices must beÂ exact</li></ul><p><strong>Trade-off:</strong> Slower but perfectly accurate</p><h3>Eventual Consistency: The â€œGood Enoughâ€Â Approach</h3><p><strong>Analogy:</strong> Gossip in a smallÂ town</p><ul><li>Someone hears news and tells a fewÂ friends</li><li>Those friends tell moreÂ friends</li><li>Eventually, everyone knows, but not at the exact sameÂ moment</li><li>For a little while, some people have the latest gossip, othersÂ donâ€™t</li></ul><p><strong>How itÂ works:</strong></p><ul><li>When you update data, it says â€œsuccessâ€ immediately</li><li>The system gradually updates all copies in the background</li><li>For a short time, different users might see different versions</li></ul><p><strong>Real-world examples:</strong></p><ul><li>â¤ï¸ Social media likes: If your like count is off by 1 for a few seconds, itâ€™sÂ fine</li><li>ğŸ“± Chat apps: Messages might arrive in slightly different order</li><li>ğŸ›’ Product catalogs: Inventory counts can be slightlyÂ delayed</li></ul><p><strong>Trade-off:</strong> Faster but temporarily inconsistent</p><h3>2.3 Distributed Systems Fundamentals: Teamwork Makes the DreamÂ Work</h3><h3>What is a Distributed System?</h3><p>Simple Definition: Instead of one superhero computer doing all the work, you have many regular computers working together as aÂ team.</p><p>Real-world Analogy:</p><ul><li>Single computer = One person trying to build a entire houseÂ alone</li><li>Distributed system = A construction crew with different workers (electrician, plumber, painter) all workingÂ together</li></ul><h3>Why Do We Need Distributed Systems?</h3><p>Problem: What happens when your app gets REALLYÂ popular?</p><ul><li>Single server = ğŸš— Toyota Corolla (good for personalÂ use)</li><li>Distributed system = ğŸš„ Bullet Train (can handle millions of passengers)</li></ul><p>Specific Reasons:</p><ol><li>Too Much Data: Canâ€™t fit all user data on oneÂ computer</li><li>Too Many Users: One computer canâ€™t handle millions ofÂ requests</li><li>Risk of Failure: If one computer dies, everything stops</li><li>Geographic Needs: Users in different countries need fastÂ access</li></ol><h3>Key Building Blocks of Distributed Systems:</h3><h4>1. Nodes = The TeamÂ Members</h4><ul><li>Each computer in the system is called aÂ â€œnodeâ€</li><li>Like employees in a company department</li></ul><h4>2. Leader Election = Choosing theÂ Boss</h4><p>The Problem: Whoâ€™s in charge when thereâ€™s noÂ manager?</p><ul><li>Scenario: The team lead goes onÂ vacation</li><li>Solution: The team automatically elects a new temporary lead</li></ul><p>How it works inÂ tech:</p><ul><li>All nodes â€œvoteâ€ for who should beÂ leader</li><li>If the leader crashes, they immediately elect a newÂ one</li><li>Example: When your Wi-Fi router restarts, all your devices automatically figure out how to reconnect</li></ul><h4>3. Data Replication = Making BackupÂ Copies</h4><p>Analogy: Important documentsâ€Šâ€”â€Šyou keep copies in office safe, bank vault, andÂ home</p><p>In distributed systems:</p><ul><li>Same data is stored on multiple computers</li><li>Why? If one computer burns down, your data is safe elsewhere</li></ul><h4>4. Fault Tolerance = The SafetyÂ Net</h4><p>Concept: The system should work even when things goÂ wrong</p><ul><li>Single system: One computer dies = Everything stopsÂ ğŸš«</li><li>Distributed system: One computer dies = Others take overÂ âœ…</li></ul><p>Real example: Google Searchâ€Šâ€”â€Šif one data center has a power outage, you can still search because other data centers handle theÂ load.</p><h3>How Distributed Systems ActuallyÂ Work:</h3><h4>The Clientâ€™sÂ View:</h4><p>You type google.com and get search results. You don&#39;t know or careÂ that:</p><ul><li>Your request went to a loadÂ balancer</li><li>Which sent it to one of thousands ofÂ servers</li><li>Which queried multiple databases</li><li>And combined results from different dataÂ centers</li></ul><p>To you, it feels like talking to one magical computer!</p><h4>The InternalÂ Reality:</h4><pre>You â†’ Load Balancer â†’ [Server A, Server B, Server C...] â†’ [Database 1, Database 2...]</pre><h3>Common Patterns in Distributed Systems:</h3><h4>Pattern 1: Master-Worker (Leader-Follower)</h4><ul><li>One master coordinates theÂ work</li><li>Many workers do the actual processing</li><li>Like: A construction site with one foreman and manyÂ workers</li></ul><h4>Pattern 2: Peer-to-Peer</h4><ul><li>All computers areÂ equal</li><li>They cooperate directly with eachÂ other</li><li>Like: A group of friends planning a tripÂ together</li></ul><h4>Pattern 3: Client-Server with Replication</h4><ul><li>Multiple servers with the sameÂ data</li><li>Requests are distributed amongÂ them</li><li>Like: Multiple customer service centers with the same information</li></ul><h3>The Big Challenges (Why This isÂ Hard):</h3><h4>1. The Coordination Problem</h4><p>Analogy: Getting 100 chefs to cook one perfect mealÂ together</p><ul><li>Timing issues</li><li>Communication failures</li><li>Different opinions</li></ul><h4>2. The Consistency Problem</h4><p>This is where CAP Theorem comesÂ in!</p><ul><li>How do you keep all copies of data theÂ same?</li><li>What happens when networksÂ fail?</li></ul><h4>3. The â€œSplit-Brainâ€ Problem</h4><p>Scenario: Two parts of the system canâ€™t talk to eachÂ other</p><ul><li>Both think they should be inÂ charge</li><li>Both start makingÂ changes</li><li>Result: Chaos and data corruption!</li></ul><h3>Real-World Examples You Use EveryÂ Day:</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/629/1*1ZnEWNsbc2Qw2g-MzZgQKQ.png\" /></figure><h3>ğŸ’¡ Why This Matters toÂ You:</h3><p>As a Developer:</p><ul><li>Youâ€™ll almost always work with distributed systems</li><li>Understanding these concepts helps you build better, more reliableÂ apps</li><li>Youâ€™ll avoid common pitfalls that crashÂ systems</li></ul><p>Simple Test: Is your system distributed?</p><ul><li>Yes if: Multiple computers workÂ together</li><li>No if: Everything runs on oneÂ machine</li></ul><p><strong>Remember: </strong>Distributed systems are like a well-coordinated sports team. Individual players are good, but together they can win championships! ğŸ†</p><h3>III. DATABASE SCALING STRATEGIES</h3><h3>3.1 Database Scaling Overview: The Step-by-Step Approach</h3><p>Think of growing a small shop into a supermarket chain:</p><p>Step 1: Make your current shop more efficient (Indexing &amp; Partitioning)<br>Step 2: Hire more staff for customer service (Master-Slave)<br>Step 3: Open multiple locations (Sharding)<br>Step 4: Choose the right business model (SQL vsÂ NoSQL)</p><p>Golden Rule: Donâ€™t over-engineer! Start simple, scale only whenÂ needed.</p><h3>3.2 Indexing: The Bookâ€™s Index for YourÂ Database</h3><p>Analogy: Finding a word in aÂ book</p><ul><li>Without index: Read every page â†’ SlowÂ â³</li><li>With index: Go to index, find page number â†’ FastÂ âš¡</li></ul><p>How Database IndexingÂ Works:</p><ul><li>Creates a separate â€œindex tableâ€ (usingÂ B-trees)</li><li>Stores column values in sortedÂ order</li><li>Lets database jump directly to data instead of scanning everything</li></ul><p>B-trees Explained Simply:</p><ul><li>Like a company organization chart</li><li>CEO â†’ Managers â†’ Team Leads â†’ Employees</li><li>Each level helps you narrow down searchÂ quickly</li></ul><p>Example:</p><pre>-- Without index: Scans 1 million rows<br>SELECT * FROM users WHERE id = 500000;<br><br>-- With index: Directly jumps to row 500000<br>CREATE INDEX idx_users_id ON users(id);<br>SELECT * FROM users WHERE id = 500000;</pre><p>Trade-off: Indexes make reads faster but slow down writes (because indexes need updating).</p><h3>3.3 Partitioning: Dividing a Big Table into SmallerÂ Tables</h3><p>Analogy: A giant filing cabinet vs multiple smallerÂ cabinets</p><ul><li>One giant cabinet: Hard to find files, heavyÂ drawers</li><li>Multiple cabinets: Organized by category, easier toÂ manage</li></ul><p>How itÂ works:</p><ul><li>Split users table into users_1, users_2,Â users_3</li><li>All partitions stay on the same databaseÂ server</li></ul><pre>BEFORE Partitioning:<br>users table (10 million rows)<br>â”‚<br>â”œâ”€â”€ user1, user2, ..., user10000000<br><br><br>AFTER Partitioning:<br>users table<br>â”‚<br>â”œâ”€â”€ users_1 (1-3 million)<br>â”œâ”€â”€ users_2 (4-6 million)  <br>â”œâ”€â”€ users_3 (7-10 million)</pre><p>Benefits:</p><ul><li>Faster queries (searching smallerÂ tables)</li><li>Easier maintenance</li><li>Can archive old partitions</li></ul><h3>3.4 Master-Slave Architecture: The Boss and Assistants</h3><p>Analogy: A restaurant kitchen</p><ul><li>Master (Head Chef): Handles all cookingÂ (writes)</li><li>Slaves (Sous Chefs): Handle food prep and platingÂ (reads)</li></ul><p>How itÂ works:</p><ul><li>Write requests â†’ Go to MasterÂ database</li><li>Read requests â†’ Distributed among Slave databases</li><li>Data replication: Master automatically copies data toÂ Slaves</li></ul><pre>CLIENTS â†’ [LOAD BALANCER] â†’ [SLAVE DB] â† [MASTER DB] â†’ [SLAVE DB]<br>                â†‘              â†‘              â†‘<br>              (Reads)        (Reads)       (Reads)<br>                                â†“<br>                            (Writes go to Master)</pre><p>Perfect for: Read-heavy applications (blogs, news sites, socialÂ media)</p><h3>3.5 Multi-master Setup: Multiple HeadÂ Chefs</h3><p>When one Master isnâ€™tÂ enough:</p><ul><li>Problem: Single Master canâ€™t handle all writeÂ traffic</li><li>Solution: Have multiple Masters that can all handleÂ writes</li></ul><p>Analogy: Multiple franchise locations of the same restaurant</p><ul><li>Each location can take ordersÂ (writes)</li><li>They sync their menus (data) with eachÂ other</li></ul><p>The Challenge: Conflict Resolution<br>Scenario: Both locations update the â€œspecial dishâ€ at the sameÂ time</p><ul><li>Location A sets it toÂ â€œPastaâ€</li><li>Location B sets it toÂ â€œPizzaâ€</li></ul><p>Solutions:</p><ol><li>â€œLast write winsâ€â€Šâ€”â€ŠUse timestamps</li><li>Custom logicâ€Šâ€”â€ŠBusiness rulesÂ decide</li><li>Merge changesâ€Šâ€”â€ŠCombine bothÂ values</li></ol><p>Use case: Global applications with users in different regions</p><h3>3.6 Database Sharding: The NuclearÂ Option</h3><p>Sharding = Partitioning + Different Servers</p><p>Analogy: A library thatâ€™s grown tooÂ big</p><ul><li>One building: Canâ€™t hold all books, hard toÂ manage</li><li>Multiple buildings: Each holds different bookÂ sections</li></ul><p>Sharding Strategies:</p><h4>1. Range-based Sharding</h4><pre>Shard 1: Users A-F    (Server in New York)<br>Shard 2: Users G-M    (Server in London)  <br>Shard 3: Users N-Z    (Server in Tokyo)</pre><p>Problem: Uneven distribution (too many â€œSâ€Â names)</p><h4>2. Hash-based Sharding</h4><pre>shard_number = hash(user_id) % 3<br># user_id=5 â†’ hash(5)=XYZ â†’ XYZ % 3 = 2 â†’ Shard 2</pre><p>Benefit: Even distribution</p><h4>3. Geographic Sharding</h4><pre>US users â†’ Shard in Virginia<br>EU users â†’ Shard in Frankfurt<br>Asia users â†’ Shard in Singapore</pre><p>Major Disadvantages of Sharding:</p><ul><li>âŒ Complex joins across shards areÂ painful</li><li>âŒ No cross-shard transactions</li><li>âŒ Hard to rebalance when adding newÂ shards</li><li>âŒ Application complexityâ€Šâ€”â€Šyou manage theÂ routing</li></ul><h3>3.7 Database Scaling Summary: Decision Framework</h3><p>Follow this simple flowchart:</p><pre>Start with single database<br>    â†“<br>Add INDEXES for slow queries<br>    â†“<br>Do PARTITIONING for large tables  <br>    â†“<br>For read-heavy traffic: MASTER-SLAVE<br>    â†“<br>For write-heavy traffic: SHARDING<br>    â†“<br>Only when absolutely necessary!</pre><p>Quick Guide:</p><ul><li>Read-heavy? â†’ Master-Slave</li><li>Write-heavy? â†’Â Sharding</li><li>Just big tables? â†’ Partitioning</li><li>Slow queries? â†’Â Indexing</li></ul><h3>3.8 SQL vs NoSQL: Complete Comparison</h3><h4>SQL Databases (MySQL, PostgreSQL)</h4><p>Like a strict government office:</p><ul><li>Fixed formsÂ (schema)</li><li>Everything must follow rulesÂ (ACID)</li><li>Great for organized data</li></ul><p>Use when:</p><ul><li>You need transactions (banking, e-commerce)</li><li>Data structure is predictable</li><li>Complex queries and joins areÂ needed</li></ul><h4>NoSQL Databases (MongoDB, Redis, Cassandra)</h4><p>Like a flexibleÂ startup:</p><ul><li>No fixed forms (schemaless)</li><li>Fast andÂ scalable</li><li>Different types for different jobs</li></ul><p>NoSQL Types:</p><ol><li>Document (MongoDB)â€Šâ€”â€ŠJSON-like documents</li><li>Key-Value (Redis)â€Šâ€”â€ŠSimple key-value pairs</li><li>Column-family (Cassandra)â€Šâ€”â€ŠOptimized for bigÂ data</li><li>Graph (Neo4j)â€Šâ€”â€ŠFor connected data (social networks)</li></ol><p>Use when:</p><ul><li>You need massiveÂ scale</li><li>Data structure changes frequently</li><li>Speed is more important than perfectÂ accuracy</li></ul><h3>ğŸ¯ Quick DecisionÂ Guide</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/695/1*eSrnZWP3teIQiZ-_D3WIYQ.png\" /></figure><p><strong>Remember: </strong>Most successful companies use a mix of these strategies. For example, use SQL for payments and NoSQL for user sessions. Choose the right tool for each job!Â ğŸ› ï¸</p><h3>IV. Architecture Patterns</h3><p>System architecture patterns define <strong>how components of a system are structured and interact</strong>.<br> Choosing the right one helps make systems more <strong>scalable, reliable, and easier to maintain</strong>.</p><p>Letâ€™s look at the most common patterns youâ€™ll encounter ğŸ‘‡</p><h3>4.1 Microservices Architecture</h3><h3>ğŸ”· Monolith vs Microservices</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*4ZKYkNomGIixnutz\" /></figure><h4>Monolithic Architecture</h4><p>All parts of the system are built and deployed together as <strong>one largeÂ unit</strong>.</p><pre>[ User Interface ]<br>        |<br>[ Application Logic ]<br>        |<br>[ Database ]</pre><ul><li>Tight coupling between components</li><li>Harder to scale orÂ modify</li><li>One bug can crash the wholeÂ system</li></ul><h4>Microservices Architecture</h4><p>The system is divided into <strong>independent, smaller services</strong> that communicate throughÂ APIs.</p><pre>+-----------------+<br>          |    API Gateway   |<br>          +--------+---------+<br>                   |<br>   +---------------+----------------+<br>   |               |                |<br>+------+       +--------+       +--------+<br>| Auth |       | Orders |       | Payment|<br>+------+       +--------+       +--------+<br>   |               |                |<br>[DB1]            [DB2]             [DB3]</pre><ul><li>Each service can be deployed or scaled independently</li><li>Failures in one service donâ€™t affectÂ others</li><li>Easier to manage with teams working inÂ parallel</li></ul><h3>Why Use Microservices?</h3><p>âœ… <strong>Scalability</strong>â€Šâ€”â€ŠScale only whatâ€™s needed<br> âœ… <strong>Flexibility</strong>â€Šâ€”â€ŠDifferent tech stacks for each service<br> âœ… <strong>Fault Isolation</strong>â€Šâ€”â€ŠOne crash doesnâ€™t kill everything<br> âœ… <strong>Faster Updates</strong>â€Šâ€”â€ŠDeploy smaller parts frequently</p><p><strong>ğŸ’¡ Try This:</strong><br> List 3 microservices that might exist in an app like <em>Swiggy</em> orÂ <em>Netflix</em>.</p><h3>API GatewayÂ Pattern</h3><p>An <strong>API Gateway</strong> acts as the single entry point between clients and your microservices.<br> It routes, filters, and secures all incoming requests.</p><pre>+---------+<br>        |  Client |<br>        +----+----+<br>             |<br>             v<br>      +-------------+<br>      | API Gateway |<br>      +------+------+  <br>             |<br>   +---------+---------+<br>   |         |         |<br>+------+ +--------+ +--------+<br>| Auth | | Orders | | Payment|<br>+------+ +--------+ +--------+</pre><p><strong>Responsibilities:</strong></p><ul><li>Request routing</li><li>Authentication</li><li>Caching</li><li>Rate limiting</li></ul><h3>4.2 Event-Driven Architecture (EDA)</h3><h3>Introduction toÂ EDA</h3><p>EDA systems communicate using <strong>events</strong>, not direct calls.<br> An <em>event</em> is something that happenedâ€Šâ€”â€Šlike <em>Order Placed</em> or <em>User Registered</em>.</p><pre>+--------------+     +---------------+     +------------------+<br>| Order Service| --&gt; |  Event Broker  | --&gt; | Notification Svc |<br>+--------------+     +---------------+     +------------------+<br>                              |<br>                              v<br>                        +-------------+<br>                        | Inventory Svc|<br>                        +-------------+</pre><h3>Simple Event Notification</h3><p>The producer just notifies others that <em>something happened</em>, without sending extraÂ details.</p><pre>[ Order Service ]<br>       |<br>  &quot;OrderCreated&quot; Event<br>       |<br>       v<br>[ Analytics Service ]<br>(fetches details later)</pre><h3>Event-Carried StateÂ Transfer</h3><p>Here, the event <strong>includes all the necessary data</strong>, so consumers donâ€™t need to ask forÂ details.</p><pre>Event: OrderCreated {<br>   order_id: 2025,<br>   user_id: 17,<br>   items: [&quot;T-shirt&quot;, &quot;Shoes&quot;]<br>}</pre><pre>[ Order Service ] --&gt; [ Inventory Svc ]<br>                           updates stock</pre><p><strong>ğŸ’¡ Try This:</strong><br> Think of an example where an event system could improve responsiveness in an app (hint: chat, notifications, payments).</p><h3>4.3 Load Balancer DeepÂ Dive</h3><h3>Why Load Balancers?</h3><p>A <strong>Load Balancer (LB)</strong> distributes incoming traffic across multiple servers to prevent overload.</p><pre>+--------+<br>           | Client |<br>           +---+----+<br>               |<br>               v<br>        +---------------+<br>        | Load Balancer  |<br>        +--+------+------+ <br>           |      |<br>   +-------+--+  +--+-------+<br>   | Server A |  | Server B |<br>   +----------+  +----------+</pre><p><strong>Benefits:</strong></p><ul><li>Improves performance</li><li>Prevents downtime</li><li>Enables scaling horizontally</li></ul><h3>Load Balancer Algorithms</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/944/1*-fIi7_Ek9Oxo_IGEe0hjBw.png\" /></figure><p><strong>ğŸ’¡ Try This:</strong><br> If one server has double CPU power, which algorithm should you use?<br> <em>(Answer: Weighted RoundÂ Robin)</em></p><h3>4.4 ProxyÂ Systems</h3><p>A <strong>Proxy</strong> acts as an intermediaryâ€Šâ€”â€Šforwarding requests or responses between clients andÂ servers.</p><h3>Forward Proxy</h3><p>Sits between the <strong>client</strong> and the <strong>internet</strong>â€Šâ€”â€Šoften used for security or contentÂ control.</p><pre>[ Client ] --&gt; [ Forward Proxy ] --&gt; [ Internet ]</pre><p><strong>Use Cases:</strong></p><ul><li>Hide clientÂ IP</li><li>Block restricted sites</li><li>Cache frequently visitedÂ pages</li></ul><h3>Reverse Proxy</h3><p>Sits between the <strong>internet</strong> and your <strong>servers</strong>â€Šâ€”â€Šhandles requests before they reach theÂ backend.</p><pre>[ Client ]<br>     |<br>     v<br>[ Reverse Proxy ]<br>     |<br>     v<br>+------------+   +------------+<br>| Server A   |   | Server B   |<br>+------------+   +------------+</pre><p><strong>Benefits:</strong></p><ul><li>Load balancing</li><li>SSL termination</li><li>Security (hides real serverÂ details)</li><li>Caching responses</li></ul><h3>Building Your Own Reverse Proxy (Conceptually)</h3><p>Hereâ€™s how a basic reverse proxy works step-by-step:</p><ol><li>Accept incoming client requests.</li><li>Determine which backend server should handleÂ it.</li><li>Forward theÂ request.</li><li>Collect and return the serverâ€™s response.</li></ol><pre>Client â†’ Reverse Proxy â†’ Server</pre><p><strong>Example (Pseudo-code):</strong></p><pre>const httpProxy = require(&#39;http-proxy&#39;);<br>const proxy = httpProxy.createProxyServer({});<br><br>require(&#39;http&#39;).createServer((req, res) =&gt; {<br>  proxy.web(req, res, { target: &#39;http://localhost:8080&#39; });<br>}).listen(3000);</pre><p><strong>ğŸ’¡ Try This:</strong><br> Why might Netflix or YouTube use reverse proxies?<br> <em>(Hint: To balance load, cache data, and protect backend servers.)</em></p><h3>âœ¨ QuickÂ Summary</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/923/1*YViCxP8ZklwUqhp7aArHnw.png\" /></figure><h3>V. Performance Optimization</h3><p>Performance optimization is all about making your system <strong>faster, more reliable, and scalable</strong>.<br> In this section, weâ€™ll explore how <strong>caching</strong>, <strong>Redis</strong>, and <strong>CDNs</strong> help reduce latency and improve user experience.</p><h3>5.1 Caching Fundamentals</h3><h3>Caching Introduction</h3><p><strong>Caching</strong> means storing frequently accessed data in a <strong>temporary memory</strong> (cache) so it can be fetched quickly without redoing expensive operations like databaseÂ queries.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*ISsVWUKSemhY9lYg\" /></figure><p>When the client requestsÂ data:</p><ol><li>If itâ€™s in the cache â†’ returned immediately (<strong>cacheÂ hit</strong>)</li><li>If not â†’ fetched from DB and then saved in cache (<strong>cacheÂ miss</strong>)</li></ol><p>This saves time, reduces database load, and speeds up responses.</p><h3>Benefits ofÂ Caching</h3><p>âœ… <strong>Speed:</strong> Cached data is retrieved much faster<br> âœ… <strong>Reduced Load:</strong> Database gets fewer requests<br> âœ… <strong>Scalability:</strong> System handles more users easily<br> âœ… <strong>Cost Savings:</strong> Less computation and bandwidth usage</p><p><strong>Example:</strong><br> When you scroll Instagram, your feed doesnâ€™t fetch posts from the main database each timeâ€Šâ€”â€Šitâ€™s served from cache (like Redis or Memcached).</p><h3>Types ofÂ Caches</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/915/1*7p08ePIOLehXGN0ITJ1ADg.png\" /></figure><pre>User â†’ Browser Cache â†’ App Cache â†’ DB Cache â†’ Database</pre><p><strong>ğŸ’¡ Try This:</strong><br> Think of a website you use daily (like YouTube). Which parts might be cached andÂ where?</p><h3>5.2 Redis DeepÂ Dive</h3><h3>Redis Introduction</h3><p><strong>Redis (Remote Dictionary Server)</strong> is an <strong>in-memory key-value database</strong> usedÂ for:</p><ul><li>Caching</li><li>Queues</li><li>Session storage</li><li>Real-time analytics</li></ul><p>Itâ€™s <strong>super fast</strong> because it keeps data in RAM instead ofÂ disk.</p><pre>[App] â†” [Redis Cache] â†” [Database]</pre><h3>Redis DataÂ Types</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/942/1*jNu91Dol9cPYkdRcS2Ec3w.png\" /></figure><p><strong>Example Commands:</strong></p><pre>SET username &quot;Rajdeep&quot;<br>GET username<br>LPUSH messages &quot;Hi&quot;<br>LRANGE messages 0 -1</pre><h3>ğŸ§° Redis Implementation Examples</h3><p><strong>Scenario:</strong> Youâ€™re building an e-commerce site.<br> When users check product prices frequently, store them inÂ Redis:</p><pre>GET product:123:price  â†’ cache miss â†’ fetch from DB â†’ save to Redis<br>GET product:123:price  â†’ cache hit â†’ serve instantly</pre><p><strong>ğŸ’¡ Try This:</strong><br> Imagine your app shows trending posts every few seconds.<br> Would Redis or a database be faster?Â Why?</p><h3>5.3 Content Delivery NetworkÂ (CDN)</h3><h3>CDN Introduction</h3><p>A <strong>Content Delivery Network (CDN)</strong> is a network of <strong>distributed servers</strong> that deliver web content (like images, videos, scripts) to users <strong>from the nearest geographic location</strong>.</p><pre>User (India) â†’ CDN Server (Mumbai)<br>User (US) â†’ CDN Server (New York)</pre><p>This ensures faster loading times and reduced latency globally.</p><h3>How CDNÂ Works</h3><ol><li><strong>User requestsÂ content</strong></li><li>CDN checks if itâ€™s cached in the nearest edgeÂ server</li></ol><ul><li>If <strong>yes</strong> â†’ serves instantly (<strong>cacheÂ hit</strong>)</li><li>If <strong>no</strong> â†’ fetches from origin server (<strong>cacheÂ miss</strong>)</li></ul><p>3. The CDN stores that file for futureÂ requests</p><pre>[Client] â†’ [Nearest CDN Node] â†’ [Origin Server]</pre><p><strong>Example:</strong><br> Platforms like YouTube, Netflix, and Amazon use CDNs so your videos load fast wherever youÂ are.</p><h3>ğŸ”‘ Key CDNÂ Concepts</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/840/1*ttHwnxiMrfVRSvWpjhHZXQ.png\" /></figure><p><strong>ğŸ’¡ Try This:</strong><br> If a file changes on your website, how can the CDN be told to serve the updated version? <em>(Hint: cache invalidation)</em></p><h3>VI. STORAGE SOLUTIONS</h3><h3>6.1 BlobÂ Storage</h3><h4>What is BlobÂ Storage?</h4><p><strong>Blob Storage (Binary Large Object Storage)</strong> is a way to store large amounts of <strong>unstructured data</strong>â€Šâ€”â€Šlike images, videos, PDFs, audio files, backups, or logsâ€Šâ€”â€Šin the cloud.<br> Unlike databases (which store structured tables and rows), blob storage simply keeps raw files in containers (like folders), each with a uniqueÂ link.</p><p>You can think of it like <strong>Google Drive for applications</strong>â€Šâ€”â€Šapps upload and retrieve large files through APIs instead of user interfaces.</p><h4>Blob StorageÂ Diagram</h4><pre>[Client/App] <br>     |<br>     v<br>[Blob Storage Container] --&gt; [Object1: image.jpg]<br>                             [Object2: video.mp4]<br>                             [Object3: backup.zip]</pre><p><strong>Example:</strong><br> When you upload a photo to Instagram:</p><ul><li>The <strong>metadata</strong> (caption, tags) might go into a database.</li><li>The <strong>photo itself</strong> is stored in blobÂ storage.</li></ul><h4>AWS S3Â Overview</h4><p><strong>Amazon S3 (Simple Storage Service)</strong> is one of the most popular blob storage services in theÂ world.</p><p>It stores data as <strong>objects</strong> inside <strong>buckets</strong> and provides featuresÂ like:</p><ul><li><strong>High availability:</strong> Your data is always accessible.</li><li><strong>Durability:</strong> Itâ€™s designed for 99.999999999% (11 nines) data durability.</li><li><strong>Scalability:</strong> Automatically handles any amount ofÂ data.</li><li><strong>Versioning:</strong> Keeps old versions of files to prevent accidental loss.</li><li><strong>Access control:</strong> Secure your data with IAM policies.</li></ul><p>ğŸ“¦ <strong>S3 Structure (Simplified):</strong></p><pre>Bucket<br> â”œâ”€â”€ image1.jpg<br> â”œâ”€â”€ report.pdf<br> â”œâ”€â”€ /videos/<br> â”‚     â””â”€â”€ demo.mp4<br> â””â”€â”€ metadata.json</pre><p>ğŸ’¡ <strong>Real-life analogy:</strong><br> S3 is like a massive, global hard drive that applications can read/write to instantly.</p><h3>6.2 Data Redundancy andÂ Recovery</h3><h4>Why Data Redundancy?</h4><p><strong>Data redundancy</strong> means storing multiple copies of the same data in different placesâ€Šâ€”â€Šso even if one server or region fails, your data remains safe.<br> This ensures <strong>high availability</strong> and <strong>disaster recovery</strong>.</p><p>There are mainly twoÂ levels:</p><ul><li><strong>Within-region redundancy:</strong> Copies exist within one data center (for quickÂ access).</li><li><strong>Cross-region redundancy:</strong> Copies exist across multiple data centers worldwide.</li></ul><p>ğŸ’¡ <strong>Example:</strong><br> If your data is stored in AWS Mumbai and that data center goes down, AWS automatically switches to the backup in Singapore.</p><h4>Data Redundancy Diagram</h4><pre>+----------------+<br>          |  Primary Data  |<br>          +----------------+<br>           /              \\<br>          v                v<br>+----------------+    +----------------+<br>| Backup Server1 |    | Backup Server2 |<br>+----------------+    +----------------+</pre><h4>Backup Strategies</h4><p>A <strong>backup strategy</strong> is a plan for regularly copying and securing data to avoidÂ loss.</p><p>Common backup strategies include:</p><ol><li><strong>Full Backup:</strong> Copy everything (slow, but complete).</li><li><strong>Incremental Backup:</strong> Copy only what changed since the last backup (faster).</li><li><strong>Differential Backup:</strong> Copy everything that changed since the last fullÂ backup.</li></ol><p><strong>Tip:</strong> Automate backups using tools like <strong>AWS Backup</strong> or <strong>cron jobs</strong> for on-premises systems.</p><p><strong>Example Schedule:</strong></p><ul><li>Daily incremental backup</li><li>Weekly fullÂ backup</li><li>Monthly archive to cold storage (e.g., S3Â Glacier)</li></ul><h4>Continuous Redundancy</h4><p><strong>Continuous redundancy</strong> means your data is constantly synchronized across multiple servers or locationsâ€Šâ€”â€Šin realÂ time.</p><p>This is often achieved using <strong>replication</strong>:</p><ul><li><strong>Synchronous replication:</strong> Data is written to all copies <em>at the same time</em> (strong consistency).</li><li><strong>Asynchronous replication:</strong> Data is written to backups <em>after</em> the main one (faster, but mayÂ lag).</li></ul><p>ğŸ’¡ <strong>Used in:</strong> Mission-critical systems like banking, e-commerce, and healthcare, where losing even a few seconds of data could be catastrophic.</p><h4>Backup ScheduleÂ Diagram</h4><pre>Day 1 -&gt; Full Backup<br>Day 2 -&gt; Incremental Backup<br>Day 3 -&gt; Incremental Backup<br>Day 7 -&gt; Full Backup</pre><h3>VII. MESSAGING &amp; COMMUNICATION</h3><p>Modern distributed systems need reliable ways for services to <strong>communicate and share data</strong>â€Šâ€”â€Šoften across different servers or even continents.<br> Thatâ€™s where <strong>messaging systems</strong> come in. They make sure data moves smoothly and efficiently between components, even if some parts are temporarily down.</p><h3>7.1 MessageÂ Brokers</h3><p>A <strong>message broker</strong> is like a <strong>post office</strong> for your services.<br> It receives messages from one service, holds them safely, and delivers them to anotherâ€Šâ€”â€Šensuring no data is lost even if the receiver is busy orÂ offline.</p><p>Examples: <strong>RabbitMQ</strong>, <strong>Apache Kafka</strong>, <strong>ActiveMQ</strong>, <strong>AmazonÂ SQS</strong>.</p><h3>Synchronous vs Asynchronous Communication</h3><h4>ğŸ”¹ Synchronous Communication</h4><ul><li>Sender waits for the receiver toÂ respond.</li><li>Works like a <strong>phone call</strong>â€Šâ€”â€Šboth must beÂ active.</li><li>Example: Service A â†’ Service B â†’ ResponseÂ back.</li><li>Used when an immediate answer is needed (e.g., loginÂ API).</li></ul><pre>Service A â†’ (Request) â†’ Service B<br>          â† (Response) â†</pre><h4>ğŸ”¹ Asynchronous Communication</h4><ul><li>Sender doesnâ€™t wait for the receiver.</li><li>Works like <strong>sending a text message</strong>â€Šâ€”â€Šthe receiver can replyÂ later.</li><li>Increases reliability and performance in distributed systems.</li></ul><pre>Service A â†’ [Message Broker] â†’ Service B</pre><h3>Why Use MessageÂ Brokers?</h3><p>âœ… <strong>Decoupling:</strong> Services can operate independently.<br> âœ… <strong>Reliability:</strong> Messages are not lost even if receivers crash.<br> âœ… <strong>Scalability:</strong> Multiple consumers can read messages in parallel.<br> âœ… <strong>Load Management:</strong> Brokers handle message queues, preventing overload.</p><p>ğŸ’­ <strong>Example:</strong><br> In an e-commerce app:</p><ul><li><strong>Order Service</strong> sends an â€œOrder Placedâ€Â message.</li><li><strong>Inventory Service</strong>, <strong>Billing Service</strong>, and <strong>Notification Service</strong> each receive that message <strong>asynchronously</strong> via aÂ broker.</li></ul><h3>7.2 Message Queues vs MessageÂ Streams</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FBjNfFa6PD_ktcgrRpwEzg.png\" /></figure><pre>Message Queue:<br>Producer â†’ [Queue] â†’ Consumer A âœ…<br><br>Message Stream:<br>Producer â†’ [Stream] â†’ Consumer A âœ…<br>                       Consumer B âœ…</pre><p><strong>ğŸ’¡ Analogy:</strong><br> A queue is like a <strong>to-do list</strong> (once a task is done, itâ€™s gone).<br> A stream is like a <strong>news feed</strong> (everyone can read the sameÂ posts).</p><h3>7.3 Apache Kafka DeepÂ Dive</h3><h3>Kafka Internals</h3><p><strong>Apache Kafka</strong> is a <strong>distributed event streaming platform</strong> designed for <strong>high-throughput, real-time data pipelines</strong>.<br> Itâ€™s built around three key concepts:</p><h4>1. Producer</h4><p>Sends messages (events) intoÂ Kafka.</p><h4>2. Topic</h4><p>A category where messages are stored (like folders).<br> Each topic is divided into <strong>partitions</strong> to allowÂ scaling.</p><h4>3. Consumer</h4><p>Reads messages from KafkaÂ topics.</p><h4>4. Broker</h4><p>A Kafka server that stores messages. A cluster can have multipleÂ brokers.</p><h4>5. Zookeeper / Controller</h4><p>Manages brokers, topics, and partitions.</p><pre>[Producer] â†’ [Kafka Topic (Partition 1, 2, 3)] â†’ [Consumer Group]</pre><p><strong>Kafka ensures:</strong></p><ul><li><strong>Durability:</strong> Messages are written toÂ disk.</li><li><strong>Scalability:</strong> Multiple consumers can process messages in parallel.</li><li><strong>Replayability:</strong> Consumers can re-read past messages.</li></ul><h3>When to UseÂ Kafka</h3><p>Use Kafka when youÂ need:</p><ul><li><strong>Real-time analytics</strong> (like dashboards, metrics)</li><li><strong>Event-driven architectures</strong></li><li><strong>Streaming data</strong> (e.g., logs, transactions)</li><li><strong>Decoupled communication</strong> between microservices</li></ul><p>ğŸ’¡ <strong>Examples:</strong></p><ul><li>Netflix uses Kafka for real-time recommendations.</li><li>LinkedIn uses Kafka for activityÂ streams.</li></ul><h3>7.4 Real-time Pub/Sub</h3><p><strong>Pub/Sub (Publish/Subscribe)</strong> is a messaging patternÂ where:</p><ul><li><strong>Publishers</strong> send messages to aÂ topic.</li><li><strong>Subscribers</strong> receive messages from that topic automatically.</li></ul><p>No direct connection is needed between sender and receiverâ€Šâ€”â€Šthe <strong>broker</strong> handles allÂ routing.</p><pre>Publisher â†’ [Topic] â†’ Subscriber A<br>                         â†’ Subscriber B</pre><p><strong>Example:</strong><br> When a new video is uploaded:</p><ul><li>The <strong>Uploader Service</strong> publishes a â€œVideoUploadedâ€ event.</li><li><strong>Notification Service</strong>, <strong>Recommendation Engine</strong>, and <strong>Analytics System</strong> each subscribe to thatÂ event.</li></ul><h3>ğŸ’¡ TryÂ This</h3><ol><li>If your application sends email notifications for every order, should you use a <strong>queue</strong> or aÂ <strong>stream</strong>?</li><li>Why might an <strong>asynchronous</strong> system scale better than a <strong>synchronous</strong> one?</li></ol><h3>VIII. ADVANCED DISTRIBUTED CONCEPTS</h3><p>Distributed systems are complex, and these advanced concepts help manage <strong>scalability, fault tolerance, and performance</strong>.</p><h3>8.1 Consistent Hashing</h3><h3>Definition</h3><p>Consistent hashing is a technique used in distributed systems to <strong>distribute data across multiple nodes</strong> in a way that <strong>minimizes data movement</strong> when nodes are added orÂ removed.</p><ul><li>Traditional hashing: hash(key) % N â†’ if N changes, almost all keys are remapped.</li><li>Consistent hashing solves this by mapping both <strong>nodes and keys onto aÂ ring</strong>.</li></ul><h3>Analogy</h3><p>Imagine a <strong>pizza deliveryÂ circle</strong>:</p><ul><li>Each house (data) gets assigned a pizza delivery person (node) based on <strong>who comes next clockwise</strong> on the delivery map (hashÂ ring).</li><li>If one delivery person leaves, only the houses served by that person need a new delivery assignment. Others remain unchanged.</li></ul><h3>ASCII Diagram</h3><pre>Hash Ring (0-360Â°)<br><br>0Â°      90Â°      180Â°     270Â°<br>       |       |        |        |<br>       A       B        C        D<br>  Key k1 --&gt; next clockwise node B<br>  Key k2 --&gt; next clockwise node C<br></pre><p><strong>Benefits:</strong></p><ul><li>Minimal data reshuffling when nodes are added/removed.</li><li>Perfect for distributed caches (like Redis Cluster).</li></ul><h3>8.2 Auto-Recoverable Systems</h3><h3>Definition</h3><p>An auto-recoverable system is a <strong>distributed system capable of detecting failures and restoring itself</strong> automatically without manual intervention.</p><ul><li>Fault tolerance is essential in large-scale distributed systems.</li><li>Recovery can include restarting nodes, reassigning tasks, or restoring data from replicas.</li></ul><h3>Leader Election</h3><p><strong>Definition:</strong><br> Leader election is a process in distributed systems where nodes <strong>elect a coordinator (leader)</strong> to manage tasks like task assignment, synchronization, or resource management.</p><p><strong>Analogy:</strong><br> Imagine a <strong>groupÂ project</strong>:</p><ul><li>Everyone in the team is equal, but someone must lead to <strong>assignÂ tasks</strong>.</li><li>If the leader leaves, the team must <strong>elect a new leader</strong> automatically.</li></ul><p><strong>ASCII Diagram:</strong></p><pre>Nodes: N1, N2, N3<br><br>   Election Round:<br>   [N1] -&gt; proposes<br>   [N2] -&gt; votes<br>   [N3] -&gt; votes<br><br>   Leader chosen: N2</pre><p><strong>Popular Algorithms:</strong></p><ul><li>Bully Algorithm</li><li>Raft Consensus Algorithm</li><li>Paxos</li></ul><h3>Orchestrator Patterns</h3><p><strong>Definition:</strong><br> An orchestrator is a system that <strong>manages and automates distributed workflows</strong>, ensuring services work together seamlessly.</p><p><strong>Analogy:</strong><br> Think of a <strong>conductor in an orchestra</strong>:</p><ul><li>Each musician (service) plays aÂ part.</li><li>The conductor (orchestrator) ensures <strong>timing and coordination</strong>.</li></ul><p><strong>Examples:</strong></p><ul><li>Kubernetes (Pods and Services orchestration)</li><li>Apache Airflow (Workflow orchestration)</li></ul><h3>8.3 Big DataÂ Tools</h3><h3>Apache SparkÂ Overview</h3><p><strong>Definition:</strong><br> Apache Spark is a <strong>distributed data processing engine</strong> designed for <strong>speed, ease of use, and generality</strong>. It handles <strong>large-scale data processing</strong> with in-memory computation.</p><p><strong>Key Features:</strong></p><ul><li>Distributed computing</li><li>Fault tolerance using <strong>RDDs (Resilient Distributed Datasets)</strong></li><li>Supports batch &amp; real-time processing</li></ul><p><strong>Analogy:</strong><br> Imagine <strong>100 chefs</strong> preparing meals in parallel in aÂ kitchen:</p><ul><li>Each chef works on a portion of ingredients (data partition).</li><li>Chef failures donâ€™t stop the meal; others take over (fault tolerance).</li></ul><p><strong>ASCII Diagram:</strong></p><pre>Data Input<br>   |<br>+-----------------+<br>| Partition 1     | --&gt; Node 1<br>| Partition 2     | --&gt; Node 2<br>| Partition 3     | --&gt; Node 3<br>+-----------------+<br>   |<br>   v<br>Processing (Map, Reduce, Filter)<br>   |<br>Output Result</pre><h3>When to Use Distributed Processing</h3><ul><li><strong>Large Data Sets:</strong> Millions to billions of rows (e.g., logs, IoTÂ data).</li><li><strong>High Throughput Requirements:</strong> Real-time analytics.</li><li><strong>Fault Tolerance Needs:</strong> Systems where node failures areÂ common.</li><li><strong>Parallelizable Tasks:</strong> Tasks that can run independently on chunks of data (e.g., MapReduce).</li></ul><p><strong>Analogy:</strong></p><ul><li>Making 1,000 sandwiches alone â†’ veryÂ slow.</li><li>Making 1,000 sandwiches with 50 people simultaneously â†’ veryÂ fast.</li><li><strong>Distributed processing = multiple nodes working in parallel.</strong></li></ul><p>Perfect! Letâ€™s create a <strong>comprehensive ASCII diagram</strong> that ties together <strong>Consistent Hashing, Auto-Recovery, Leader Election, Orchestration, and Spark processing</strong> in one flow. This will visualize <strong>how an advanced distributed system works end-to-end</strong>.</p><pre>                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>                           â”‚       Clients / Users   â”‚<br>                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>                                        â”‚<br>                                        â”‚ Requests / Data<br>                                        â–¼<br>                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>                           â”‚    Consistent Hashing   â”‚<br>                           â”‚  (Distribute keys/data) â”‚<br>                           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜<br>                                   â”‚         â”‚<br>                                   â”‚         â”‚<br>                                   â–¼         â–¼<br>                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>                           â”‚ Node 1  â”‚   â”‚ Node 2  â”‚<br>                           â”‚         â”‚   â”‚         â”‚<br>                           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜<br>                                â”‚             â”‚<br>                                â”‚ Partition / Tasks<br>                                â–¼             â–¼<br>                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>                         â”‚ Apache Sparkâ”‚  â”‚ Apache Sparkâ”‚<br>                         â”‚ Executor    â”‚  â”‚ Executor    â”‚<br>                         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜<br>                               â”‚                â”‚<br>             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>             â”‚                                                    â”‚<br>             â–¼                                                    â–¼<br>      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>      â”‚ Processing  â”‚                                   â”‚ Processing  â”‚<br>      â”‚ (Map/Reduce â”‚                                   â”‚ (Map/Reduce â”‚<br>      â”‚ /Filter)    â”‚                                   â”‚ /Filter)    â”‚<br>      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜<br>            â”‚                                                 â”‚<br>            â”‚ Result Aggregation / Output                     â”‚<br>            â–¼                                                 â–¼<br>      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>      â”‚        Orchestrator       â”‚<br>      â”‚ - Task scheduling         â”‚<br>      â”‚ - Resource management     â”‚<br>      â”‚ - Workflow coordination   â”‚<br>      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>                  â”‚<br>                  â–¼<br>      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>      â”‚ Auto-Recovery &amp; Leader    â”‚<br>      â”‚ Election Mechanisms       â”‚<br>      â”‚ - Detect failed nodes     â”‚<br>      â”‚ - Restart / Reassign tasksâ”‚<br>      â”‚ - Elect new leaders       â”‚<br>      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</pre><h3>Flow Explanation</h3><ol><li><strong>Clients</strong> send requests or data to theÂ system.</li><li><strong>Consistent Hashing</strong> distributes data across nodes to ensure minimal reshuffling if nodes join orÂ leave.</li><li>Each <strong>node</strong> holds data partitions and runs <strong>Spark executors</strong> for parallel computation.</li><li><strong>Spark executors</strong> process data (Map/Reduce/Filter) and sendÂ results.</li><li>The <strong>Orchestrator</strong> ensures all nodes/tasks are coordinated and workflow isÂ smooth.</li><li><strong>Auto-Recovery &amp; Leader Election</strong> monitorÂ nodes:</li></ol><ul><li>Restart failedÂ nodes</li><li>Reassign tasks</li><li>Elect a new leader if the coordinator fails</li></ul><p>This diagram shows a <strong>full loop</strong>: data distribution â†’ processing â†’ orchestration â†’ fault-tolerance.</p><h3>IX. PRACTICAL IMPLEMENTATION</h3><p>This section focuses on <strong>applying theoretical distributed system concepts</strong> in <strong>real-world, hands-on exercises</strong>, using modern tools like Node.js, Redis, and reverseÂ proxies.</p><h3>9.1 Hands-On Exercises</h3><p>Hands-on exercises are designed to <strong>reinforce concepts</strong> through <strong>practice</strong>, rather than just theory. They usually cover <strong>deployment, configuration, and coding challenges</strong>.</p><h3>Deployment Exercises</h3><p>Deployment exercises involve <strong>installing and running distributed applications</strong> on real or virtual servers, simulating real-world environments.</p><p><strong>Analogy:</strong><br> Think of deployment as <strong>setting up a foodÂ stall</strong>:</p><ul><li>You need the stall (server), ingredients (application code), and arrangement (configurations) to start serving customers (users).</li></ul><p><strong>Typical Tasks:</strong></p><ul><li>Deploying a Node.js application on aÂ server</li><li>Deploying Docker containers</li><li>Deploying services to cloud platforms like AWS, Azure, orÂ Vercel</li></ul><p><strong>ASCII Diagram:</strong></p><pre>[Developer Code] ---&gt; [Server / VM] ---&gt; [Users Access App]</pre><h3>Configuration Exercises</h3><p>Configuration exercises focus on <strong>setting up system parameters and environment settings</strong> to optimize performance, security, and reliability.</p><p><strong>Analogy:</strong></p><ul><li>Like <strong>setting oven temperature, spice level, and serving size</strong> beforeÂ cooking.</li></ul><p><strong>Typical Tasks:</strong></p><ul><li>Setting environment variables (NODE_ENV=production)</li><li>Configuring caching strategies (Redis)</li><li>Configuring load balancers or reverseÂ proxies</li></ul><h3>Coding Challenges</h3><p>These exercises are about writing <strong>actual code</strong> to implement distributed system features.</p><p><strong>Analogy:</strong></p><ul><li>Like <strong>practicing recipes</strong> repeatedly to perfect taste andÂ timing.</li></ul><p><strong>Examples:</strong></p><ul><li>Implement caching for faster data retrieval</li><li>Implement load balancing logic</li><li>Write REST APIs that interact with multiple nodes/services</li></ul><h3>9.2 Quick LearningÂ Checks</h3><p>Quick learning checks are <strong>mini-assessments or quizzes</strong> to ensure understanding after each practical exercise.</p><p><strong>Purpose:</strong></p><ul><li>Identify gaps in knowledge immediately</li><li>Reinforce learning</li><li>Prepare for real-world implementation</li></ul><p><strong>Analogy:</strong></p><ul><li>Like <strong>tasting your dish while cooking</strong> to check if seasoning or cooking time needs adjustment.</li></ul><h3>9.3 Node.js Implementation Examples</h3><p>Node.js is often used in distributed systems because of its <strong>non-blocking, event-driven architecture</strong>, which is perfect for <strong>high-concurrency applications</strong>.</p><h3>Redis CachingÂ Code</h3><p>Redis caching involves storing frequently accessed data <strong>in memory</strong> to reduce database load and improve responseÂ times.</p><p><strong>Node.js Example (Simplified):</strong></p><pre>const redis = require(&quot;redis&quot;);<br>const client = redis.createClient();<br>client.connect();<br>async function getCachedData(key) {<br>    const cache = await client.get(key);<br>    if (cache) {<br>        console.log(&quot;Cache Hit!&quot;);<br>        return JSON.parse(cache);<br>    } else {<br>        console.log(&quot;Cache Miss!&quot;);<br>        const data = { message: &quot;Hello World&quot; }; // Simulate DB fetch<br>        await client.set(key, JSON.stringify(data), { EX: 60 }); // 60 sec TTL<br>        return data;<br>    }<br>}<br>// Usage<br>getCachedData(&quot;greeting&quot;).then(console.log);</pre><p><strong>ASCII Flow:</strong></p><pre>[Client Request] <br>      â”‚<br>      â–¼<br>[Redis Cache] -- Hit? --&gt; [Return Cached Data]<br>      â”‚ No<br>      â–¼<br>[Database] --&gt; [Cache Updated] --&gt; [Return Data]</pre><h3>Reverse ProxyÂ Code</h3><p>A reverse proxy sits <strong>between clients and backend servers</strong>, forwarding requests and improving <strong>scalability, security, and load balancing</strong>.</p><p><strong>Node.js Example using </strong><strong>http-proxy-middleware:</strong></p><pre>const express = require(&quot;express&quot;);<br>const { createProxyMiddleware } = require(&quot;http-proxy-middleware&quot;);<br>const app = express();<br>// Forward requests to backend server<br>app.use(&quot;/api&quot;, createProxyMiddleware({ <br>    target: &quot;http://localhost:5000&quot;, <br>    changeOrigin: true <br>}));<br>app.listen(3000, () =&gt; console.log(&quot;Proxy running on port 3000&quot;));</pre><p><strong>ASCII Flow:</strong></p><pre>[Client Request] ---&gt; [Reverse Proxy] ---&gt; [Backend Server]<br>                        â”‚<br>                        â–¼<br>              Load Balancing / Security</pre><p><strong>Benefits of ReverseÂ Proxy:</strong></p><ul><li>Distributes load across multiple backendÂ servers</li><li>Adds security layer (hides serverÂ details)</li><li>Can handle caching, compression, and SSL termination</li></ul><p>Perfect! Letâ€™s create a <strong>comprehensive ASCII diagram</strong> showing an <strong>end-to-end practical implementation flow</strong> in a distributed Node.js system using <strong>deployment, configuration, Redis caching, and reverse proxy</strong>. This will visualize <strong>how requests flow through the system in practice</strong>.</p><pre>                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>                      â”‚       Clients         â”‚<br>                      â”‚ (Browser / App / API) â”‚<br>                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>                                 â”‚ Requests<br>                                 â–¼<br>                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>                      â”‚   Reverse Proxy / LB  â”‚<br>                      â”‚ - Forward requests    â”‚<br>                      â”‚ - Load balancing      â”‚<br>                      â”‚ - Security layer      â”‚<br>                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>                                 â”‚<br>              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>              â”‚                                     â”‚<br>              â–¼                                     â–¼<br>     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>     â”‚  Node.js Backend   â”‚                 â”‚  Node.js Backend  â”‚<br>     â”‚  Server 1          â”‚                 â”‚  Server 2         â”‚<br>     â”‚  - Handles API     â”‚                 â”‚  - Handles API    â”‚<br>     â”‚  - Application Logicâ”‚                â”‚  - Application Logicâ”‚<br>     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>                 â”‚                                     â”‚<br>                 â”‚ Cache Check / Update                â”‚ Cache Check / Update<br>                 â–¼                                     â–¼<br>           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>           â”‚   Redis Cache â”‚&lt;--- Syncs with ---&gt;â”‚   Redis Cache â”‚<br>           â”‚ - Store hot   â”‚                    â”‚ - Store hot   â”‚<br>           â”‚   data        â”‚                    â”‚   data        â”‚<br>           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜<br>                 â”‚                                     â”‚<br>                 â–¼                                     â–¼<br>           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br>           â”‚   Database    â”‚                    â”‚   Database    â”‚<br>           â”‚ - Persistent  â”‚                    â”‚ - Persistent  â”‚<br>           â”‚   Storage     â”‚                    â”‚   Storage     â”‚<br>           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</pre><h3>Flow Explanation</h3><ol><li><strong>Clients</strong> send requests (API calls, web requests, or mobile appÂ calls).</li><li><strong>Reverse Proxy / Load Balancer</strong>:</li></ol><ul><li>Receives all clientÂ requests</li><li>Routes them to <strong>available Node.js backendÂ servers</strong></li><li>Provides <strong>security, caching, and load balancing</strong></li></ul><p><strong>3. Node.js BackendÂ Servers</strong>:</p><ul><li>Handles business logic and API processing</li><li>Checks <strong>Redis cache</strong> for requested data</li></ul><p><strong>4. RedisÂ Cache</strong>:</p><ul><li>If <strong>cache hit</strong>, returns data immediately</li><li>If <strong>cache miss</strong>, fetches data from the <strong>database</strong> and updatesÂ cache</li></ul><p><strong>5. Database</strong>:</p><ul><li>Stores persistent data that is too large or critical for in-memory caching</li></ul><p><strong>6. Auto-Scaling / Configuration</strong>:</p><ul><li>Backend servers can scale horizontally if traffic increases</li><li>Configuration ensures proper environment variables, logging, and monitoring</li></ul><h3>Key ConceptsÂ Shown</h3><ul><li><strong>Deployment</strong> â†’ Backend servers running Node.jsÂ apps</li><li><strong>Configuration</strong> â†’ Environment variables, cache TTL, reverse proxyÂ setup</li><li><strong>Redis Caching</strong> â†’ Improves performance by reducing databaseÂ load</li><li><strong>Reverse Proxy / Load Balancer</strong> â†’ Distributes traffic, improves fault tolerance</li><li><strong>Coding Challenges / Exercises</strong> â†’ Implementing cache logic, proxy rules, API endpoints</li></ul><h3>X. PROBLEM-SOLVING FRAMEWORK</h3><p>This section teaches <strong>how to approach distributed systems or system design problems effectively</strong>, step byÂ step.</p><h3>10.1 How to Solve Any System DesignÂ Problem</h3><p>A system design problem is a <strong>real-world engineering challenge</strong> where you need to <strong>design scalable, fault-tolerant, and performant systems</strong>.</p><p><strong>Analogy:</strong></p><ul><li>Think of designing a <strong>cityâ€™s transportation system</strong>:</li><li>Roads (network)</li><li>Buses/trains (services)</li><li>Stations (nodes)</li><li>Traffic lights (orchestrators /Â rules)</li></ul><p><strong>Key Principles:</strong></p><ol><li><strong>Understand the requirements clearly</strong>â€Šâ€”â€Šwhat problem are youÂ solving?</li><li><strong>Identify constraints</strong>â€Šâ€”â€Šlatency, throughput, budget, scalability.</li><li><strong>Define core components</strong>â€Šâ€”â€Šdatabases, caching, load balancers, queues.</li><li><strong>Consider trade-offs</strong>â€Šâ€”â€Šconsistency vs availability, complexity vs simplicity.</li></ol><h3>10.2 Step-by-Step Approach</h3><p><strong>Step 1: Clarify Requirements</strong></p><ul><li>Functional: What features areÂ needed?</li><li>Non-functional: Scalability, reliability, latency.</li></ul><p><strong>Step 2: Define System APIs / Interfaces</strong></p><ul><li>What endpoints will clientsÂ use?</li><li>What data flows through theÂ system?</li></ul><p><strong>Step 3: High-Level Design</strong></p><ul><li>Sketch main components: clients, servers, databases, cache.</li><li>Use ASCII or block diagrams forÂ clarity.</li></ul><p><strong>Step 4: Deep Dive Components</strong></p><ul><li>Database selection (SQL vsÂ NoSQL)</li><li>Caching strategy (Redis, Memcached)</li><li>Load balancing and reverseÂ proxies</li><li>Queueing (Kafka, RabbitMQ)</li></ul><p><strong>Step 5: Consider Bottlenecks andÂ Scaling</strong></p><ul><li>Identify potential high-load areas</li><li>Plan horizontal/vertical scaling</li></ul><p><strong>Step 6: Address Fault Tolerance &amp;Â Recovery</strong></p><ul><li>Leader election</li><li>Auto-recovery</li><li>Replication and redundancy</li></ul><p><strong>Step 7: Summarize Trade-offs and Justify Decisions</strong></p><ul><li>Why certain components were chosen overÂ others</li></ul><p><strong>ASCII Diagram of Step-by-Step Flow</strong></p><pre>Requirements -&gt; APIs -&gt; High-Level Design -&gt; Component Deep Dive<br>                 â”‚                â”‚<br>                 â–¼                â–¼<br>            Bottlenecks       Fault Tolerance<br>                 â”‚                â”‚<br>                 â””â”€â”€â”€â”€â”€â”€â†’ Trade-offs &amp; Justification</pre><h3>10.3 Common Patterns and Anti-patterns</h3><p><strong>Patterns (Best Practices):</strong></p><ul><li><strong>CQRS</strong>: Separate read/write operations for efficiency</li><li><strong>Event Sourcing</strong>: Capture state changes as a sequence ofÂ events</li><li><strong>Pub/Sub Messaging</strong>: Decouples producers and consumers</li><li><strong>Load Balancing</strong>: Distribute trafficÂ evenly</li></ul><p><strong>Anti-patterns (Pitfalls toÂ Avoid):</strong></p><ul><li><strong>God Class</strong>: Single component does everything â†’ hard toÂ scale</li><li><strong>Hard-coded Scaling</strong>: Not planning for dynamicÂ growth</li><li><strong>Ignoring Failures</strong>: No recovery, noÂ retries</li><li><strong>Overengineering</strong>: Complex solutions when simpler onesÂ suffice</li></ul><p><strong>Analogy:</strong></p><ul><li>Patterns are like <strong>highways and traffic rules</strong>â€Šâ€”â€Šthey make traffic flowÂ smooth</li><li>Anti-patterns are <strong>roadblocks or confusing intersections</strong>â€Šâ€”â€Šthey cause congestion and accidents</li></ul><h3>XI. SUMMARY &amp; NEXTÂ STEPS</h3><p>This section wraps up the course and guides future learning.</p><h3>11.1 Key Takeaways</h3><ul><li>Distributed systems require <strong>scalability, fault tolerance, and performance</strong></li><li>Concepts like <strong>consistent hashing, leader election, orchestration, caching, and reverse proxies</strong> are foundational</li><li>Practical exercises are essential for <strong>reinforcing theory</strong></li><li>Problem-solving is systematic: <strong>clarify â†’ design â†’ optimize â†’Â justify</strong></li></ul><h3>11.2 Learning Path Recommendations</h3><ol><li><strong>Start with Core Concepts:</strong></li></ol><ul><li>Networking basics, OS concepts, databases</li></ul><p><strong>2. Master Distributed System Patterns:</strong></p><ul><li>Caching, replication, messaging, orchestration</li></ul><p><strong>3. Hands-On Practice:</strong></p><ul><li>Build mini-projects with Node.js, Redis, Kafka, Docker, Kubernetes</li></ul><p><strong>4. System Design Interviews / Challenges:</strong></p><ul><li>Solve real problems using the step-by-step framework</li></ul><h3>11.3 Additional Resources</h3><ul><li><strong>Books:</strong></li><li><em>Designing Data-Intensive Applications</em>â€Šâ€”â€ŠMartin Kleppmann</li><li><em>Site Reliability Engineering</em>â€Šâ€”â€ŠGoogle SREÂ Team</li><li><strong>Websites:</strong></li><li><a href=\"https://github.com/donnemartin/system-design-primer\">SystemDesignPrimer</a></li><li><strong>Tools toÂ Explore:</strong></li><li>Redis, Kafka, Spark, Node.js, Docker, Kubernetes</li><li><strong>Courses /Â Videos:</strong></li><li>YouTube: GOTO Conferences, Tech Dummies, TechWorld withÂ Nana</li></ul><p><strong>End of the blog</strong><br> Congratulations if you read till the end!Â ğŸ‰</p><p>It took me a lot of time to put this together, and I hope you enjoyed reading it. The best way to truly learn is to <strong>implement everything yourself</strong>â€Šâ€”â€Šdonâ€™t forget to try out the exercises I included for hands-on practice.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cc7c040e3514\" width=\"1\" height=\"1\" alt=\"\">",
      "url": "https://rajdeep01.medium.com/system-design-made-simple-a-complete-beginners-guide-cc7c040e3514?source=rss-289e64b56868------2",
      "publishedAt": "Sat, 04 Oct 2025 19:47:52 GMT",
      "updatedAt": "Sat, 04 Oct 2025 19:47:52 GMT",
      "tags": [
        "software-engineering",
        "high-level-design",
        "system-design-concepts",
        "system-design-interview",
        "software-development"
      ],
      "author": "Rajdeep Singh",
      "image": "https://cdn-images-1.medium.com/max/1024/0*p2rGe_CoFyaoEgZF"
    },
    {
      "id": "https://medium.com/p/deb32cf83f04",
      "title": "From AlphaGo to ChatGPT: How Sam Altman Outran Google in the AI Race",
      "description": "",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*xE5bnn2YeejIoMVj\" /></figure><p>It was a move that shocked the world, but it didnâ€™t happen on a rocket launchpad or in a sports stadium. It unfolded on a 19x19 grid, the ancient board of the game Go. In 2016, Googleâ€™s artificial intelligence, AlphaGo, defeated 18-year-old world champion Lee Sedol. The machine had conquered what was considered the final frontier of human intellect.</p><p>Go is not chess. While chess is the game of kings, Go is the game of gods. Its complexity is mind-bending. After the first two moves in chess, there are 400 possible positions. In Go, there are over 129,000. In fact, there are more possible games of Go than there are atoms in the entire known universe. For decades, experts believed a machine could never master its intuitive depth. Yet, in 2016, itÂ did.</p><p>Googleâ€™s DeepMind team had just pulled off the greatest AI demonstration in history. They had the talent, the data, and a war chest of $90 billion. The path to dominating the future of intelligence seemed paved. But then, something strange happened. Google hesitated. And in that moment of corporate caution, the entire trajectory of technological history was rewritten.</p><h3>The Breakthrough That GoogleÂ Buried</h3><p>In 2017, eight researchers within Google published a research paper with a deceptively simple title: â€œAttention Is All You Need.â€ It contained the blueprint for the â€œtransformer architecture,â€ a revolutionary AIÂ model.</p><p>Before transformers, AI had a severe case of short-term memory loss. It processed words one by one, forgetting the beginning of a sentence by the time it reached the end. The transformer changed everything. It allowed AI to analyze all words in a sentence simultaneously, understanding context and relationships with human-like precision. It was the key to true language comprehension.</p><p>Google had invented the engine for the AI revolution. And then, it left it in theÂ garage.</p><h3>The Paralysis of aÂ Giant</h3><p>Why would a company built on â€œmoonshotâ€ projects shy away from its own greatest innovation? The answer lies in the burdens of being a $500 billionÂ empire.</p><ol><li>The Fear of Failure: Googleâ€™s business is built on trust. Billions rely on its search results daily. Executives were terrified. What if their AI chatbot gave racist, dangerous, or misleading advice? A single misstep could trigger a global backlash, congressional hearings, and a stock market crash. The headlines wrote themselves: â€œGoogleâ€™s AI CausesÂ Crisis.â€</li><li>The Cannibalization Conundrum: This was the deeper, more existential fear. Googleâ€™s revenueâ€Šâ€”â€Š$95 billion a yearâ€Šâ€”â€Šflowed from search advertising. If people could simply ask an AI a question and get a perfect answer, why would they ever â€œGoogleâ€ again? Launching a revolutionary AI product meant potentially destroying their own legendary businessÂ model.</li></ol><p>So, Google did what safe corporations do: it published papers, won academic accolades, and moved cautiously. It prioritized protecting its present over claiming theÂ future.</p><h3>The Rebels in a San Francisco Office</h3><p>Meanwhile, in a modest San Francisco office, a group of outsiders was watching. Led by a young entrepreneur named Sam Altman, OpenAI was originally a non-profit â€œdream teamâ€ assembled in 2015 by Altman, Elon Musk, and others to counter Googleâ€™s looming AI monopoly.</p><p>Their journey was far from easy. Musk departed after a fight for control, taking promised funding with him. OpenAI was bleeding cash, facing computational costs that were doubling every few months. They were on the brink of collapse, saved only by a controversial pivot to a â€œcapped-profitâ€ model and, ultimately, a lifesaving partnership with Microsoft.</p><p>Microsoftâ€™s billions provided the fuel Google had but was afraid to use. Unburdened by a legacy search business and with a â€œbias for action,â€ Altmanâ€™s team did what Google would not: they shipped aÂ product.</p><h3>The iPhone Moment ofÂ AI</h3><p>In November 2022, while Google was still refining its safeguards, OpenAI launched ChatGPT to the public. The world responded instantly. It was an â€œiPhone momentâ€â€Šâ€”â€Šan intuitive, powerful technology that everyone suddenly understood they needed. ChatGPT became the fastest-growing consumer app inÂ history.</p><p>Googleâ€™s hesitation had created a vacuum, and OpenAI rushed in to fill it. The company that owned the breakthrough was now playing catch-up.</p><h3>The Unfinished Story</h3><p>The lesson is a classic tale of innovation: incumbents are often paralyzed by their own success, while agile outsiders, with nothing to lose and everything to gain, can change theÂ world.</p><p>Today, the race is fiercer than ever. Google has unleashed Gemini, and other rivals like Claude and DeepSeek are close behind. OpenAI, despite its meteoric rise, faces immense pressure and financial challenges.</p><p>The question is no longer <em>if</em> AI will redefine our world, but <em>who</em> will lead that charge. The story of OpenAIâ€™s ascent is a powerful reminder that in the realm of technology, the future doesnâ€™t always belong to the strongest or the richest. Sometimes, it belongs to the boldest. And in 2022, Sam Altman and his team were the only ones brave enough to press â€œlaunch.â€</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=deb32cf83f04\" width=\"1\" height=\"1\" alt=\"\">",
      "url": "https://rajdeep01.medium.com/from-alphago-to-chatgpt-how-sam-altman-outran-google-in-the-ai-race-deb32cf83f04?source=rss-289e64b56868------2",
      "publishedAt": "Fri, 26 Sep 2025 11:48:03 GMT",
      "updatedAt": "Fri, 26 Sep 2025 11:48:03 GMT",
      "tags": [
        "chatbots",
        "google",
        "chatgpt",
        "ai",
        "sam-altman"
      ],
      "author": "Rajdeep Singh",
      "image": "https://cdn-images-1.medium.com/max/1024/0*xE5bnn2YeejIoMVj"
    },
    {
      "id": "https://medium.com/p/aefb128d92d2",
      "title": "Rust: From Elevator Fixes to Big Techâ€™s Quiet Backbone",
      "description": "",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*JcYPa59KUKaWSTNz\" /></figure><p>Many moons ago, if you heard the word <em>Rust</em>, youâ€™d probably think of either a survival video game or the reddish-brown process of iron oxidization. But after spending months in the malloc log, youâ€™d know it as something else entirelyâ€Šâ€”â€Š<strong>a programming language</strong> that sneaked its way from fixing an elevator to powering production servers at some of the biggest tech companies on theÂ planet.</p><p>The language famous for â€œ$0 at checkout abstractions.â€<br> The language infamous for its <em>steep learning curve</em>.<br> And lately, the language quietly proving the hatersÂ wrong.</p><h3>Rust Wasnâ€™t Supposed to Work (But ItÂ Did)</h3><p>Admittedly, Rust was dismissed not too long ago for not being widely used in production (which, compared to mainstream languages, is technically true).But hereâ€™s the catch: <strong>getting a new language adopted is brutallyÂ hard.</strong></p><p>Just ask Googleâ€™s so-called <em>C++ killer</em>, Carbon. Hyped up, polished slides, GitHub repoâ€¦ and then? Silence. It didnâ€™t even make it past the â€œcool announcementâ€ phase.</p><p>Meanwhile, Rust has been grinding. In 2022, only 9% of developers reported using it, according to Stack Overflowâ€™s survey. That number has since climbed to nearly 15%. For perspective, OCamlâ€Šâ€”â€Šbetter known as Jane Streetâ€™s employee DRMâ€Šâ€”â€Šjumped from 0.59% to 1.2%. (Congrats?)</p><p>When you strip away the memes, the hair dye, and the Twitter flame wars, hereâ€™s whatâ€™s left:<br> ğŸ‘‰ <strong>A blazingly fast, memory-safe, developer-delighting language.</strong></p><h3>Big Techâ€™s RustÂ Agenda</h3><p>Rust didnâ€™t just survive the hype cycle. It quietly infiltrated <strong>critical infrastructure</strong> across bigÂ tech:</p><ul><li><strong>Amazon</strong> built Firecracker in Rust, which powers AWS Lambda andÂ Fargate.</li><li><strong>Microsoft</strong> is rewriting parts of Windows and Azure in Rust (yes, your blue screens will now renderÂ faster).</li><li><strong>Google</strong> added Rust to Chromium andÂ Android.</li><li><strong>Meta</strong> uses it for Mononoke, their Git alternative.</li><li><strong>Discord</strong> rebuilt their backend in Rust (so when you say â€œez clapâ€ at 3 AM, it actually delivers).</li><li><strong>Dropbox</strong> moved file sync logic toÂ Rust.</li><li><strong>Twitter/X</strong> runs a Rust-based ML servingÂ layer.</li></ul><p>And hereâ€™s the thing: <strong>big tech doesnâ€™t adopt tech because itâ€™s trendy.</strong> They adopt it because it increases shareholder value. If youâ€™re rewriting your backend, core OS code, or browser engine in a new languageâ€Šâ€”â€Šit better be safe, fast, and built toÂ last.</p><h3>Beyond Shareholders: Rust in theÂ Open</h3><p>But Rust isnâ€™t just for trillion-dollar companies. The open-source and Linux world has also embracedÂ it:</p><ul><li><strong>Linux kernel</strong> now supportsÂ Rust.</li><li><strong>Ubuntu</strong> announced that core utilities (like sudo) will soon run inÂ Rust.</li><li><strong>Valve</strong> has been backing Rust in Proton forÂ years.</li><li><strong>Pythonâ€™s ecosystem</strong> is getting rusty tooâ€Šâ€”â€ŠUV, the new package manager, is written in Rust, finally replacing pipâ€™s dependency circus.</li><li><strong>Figma</strong> uses Rust to render massive design files 3Ã—Â faster.</li><li><strong>Shopify</strong> runs untrusted partner code in Rust sandboxes.</li></ul><p>Rust even compiles to WebAssembly, letting you ship safe, fast code straight to theÂ browser.</p><h3>The Elephant in the Room: Rustâ€™s LearningÂ Curve</h3><p>Letâ€™s be real: Rust isnâ€™t the easiest language toÂ learn.</p><p>Most complaints about its â€œlearning curveâ€ come from developers used to ignoring memory safety until production crashes force them to care. If youâ€™re coming from <strong>C++ or Zig</strong>, Rust will feel natural (and honestly, easier).</p><p>If youâ€™re coming from high-level land (JavaScript, Python, Ruby), expect some frustration. But alsoâ€Šâ€”â€Šexpect to walk away with skills that make you a <em>better programmer overall</em>.</p><p>Rust isnâ€™t just a low-level language. Itâ€™s a high-level language with low-level capabilities. And learning it is an investment that paysÂ off.</p><h3>Soâ€¦ Should You LearnÂ Rust?</h3><p>Iâ€™m not here to scream:<br> ğŸš€ <em>â€œRust is the future!â€</em><br> ğŸ’€ <em>â€œLearn Rust or perish!â€</em><br> ğŸ“œ <em>â€œTop 10 reasons Rust will change yourÂ life!â€</em></p><p>No. CalmÂ down.</p><p>Hereâ€™s the truth:<br> Rust is a <strong>cool, growing technology</strong> with serious momentum. If you enjoy it and have a project where it fits, go for it. Donâ€™t overthink ROI. Donâ€™t become one of those sigma-grindset ROI bros calculating if Rust adds â€œalpha.â€ Just build coolÂ stuff.</p><p>Learning is a privilege, and if youâ€™ve got the time and space to pick up Rustâ€Šâ€”â€ŠtakeÂ it.</p><h3>Final Thoughts</h3><p>Rust started as a joke, got memed to death, and still managed to become the backbone of critical infrastructure. Whether youâ€™re running cloud services, fixing the Linux kernel, or just trying to make Discord stop lagging, chances are thereâ€™s Rust under theÂ hood.</p><p>And thatâ€™s the real story: not hype, not memesâ€Šâ€”â€Š<strong>just a stable, reliable, robust programming language with a brightÂ future.</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=aefb128d92d2\" width=\"1\" height=\"1\" alt=\"\">",
      "url": "https://rajdeep01.medium.com/rust-from-elevator-fixes-to-big-techs-quiet-backbone-aefb128d92d2?source=rss-289e64b56868------2",
      "publishedAt": "Sun, 21 Sep 2025 22:26:08 GMT",
      "updatedAt": "Sun, 21 Sep 2025 22:26:08 GMT",
      "tags": [
        "c-sharp-programming",
        "web-development",
        "web-design",
        "web3",
        "rust"
      ],
      "author": "Rajdeep Singh",
      "image": "https://cdn-images-1.medium.com/max/1024/0*JcYPa59KUKaWSTNz"
    },
    {
      "id": "https://medium.com/p/7181b2e36f33",
      "title": "Why C and C++ Donâ€™t Have Garbage Collection (And Why Thatâ€™s Not Always a Bad Thing)",
      "description": "",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*ICzkJjrvj0abaMIR\" /></figure><p>If youâ€™ve ever programmed in <strong>Java, Python, or Ruby</strong>, you know the sweet relief of <em>not worrying about memory cleanup</em>. These languages have <strong>automatic garbage collection (GC)</strong>â€Šâ€”â€Šthey free memory when itâ€™s no longerÂ needed.</p><p>But if you step into <strong>C or C++</strong>, that comfort disappears. Suddenly, itâ€™s your job to free() every piece of heap memory you allocate. Forget once, and your program leaks memory like a dripping tap. Free twice, and you risk crashing theÂ program.</p><p>So naturally, the questionÂ arises:</p><p>ğŸ‘‰ <em>â€œWhy donâ€™t C and C++ just have garbage collection?â€</em></p><p>Letâ€™s break thisÂ down.</p><h3>1. What Happens to Memory When a ProgramÂ Ends?</h3><p>A common beginner misconception is that you need to manually clean up memory at the end of a program. Otherwise, people worry, your system will run out ofÂ memory.</p><p>But hereâ€™s theÂ truth:</p><ul><li>When your program <strong>terminates</strong>, the <strong>operating system (OS)</strong> steps in and cleans everything upâ€Šâ€”â€Šstack, heap, variables, instructions, all ofÂ it.</li><li>This happens automatically, regardless of whether you called free() orÂ delete.</li></ul><p>So why do we bother freeing memory in the first place?<br> Because most programs donâ€™t just run for a few seconds. Someâ€Šâ€”â€Šweb servers, databases, background daemonsâ€Šâ€”â€Šrun for <em>days, weeks, evenÂ months</em>.</p><p>If you donâ€™t free memory inside such long-running programs, memory usage keeps growing until the program slows to a crawl or crashes. Thatâ€™s why <strong>memory management matters during execution, not just at programÂ exit</strong>.</p><h3>2. How Garbage Collection Works in Other Languages</h3><p>Garbage collection is all about <strong>reachability</strong>.</p><p>Take a simple linkedÂ list:</p><ul><li>The head pointer refers to the firstÂ node.</li><li>Each node links to theÂ next.</li><li>As long as something points to a node, itâ€™s considered â€œalive.â€</li></ul><p>In Java or Python, the runtime tracks references:</p><ul><li>Each object has a reference count.</li><li>When no variable points to an object anymore, the count drops toÂ zero.</li><li>At that point, the garbage collector can safely reclaim theÂ memory.</li></ul><p>This makes programming safer and less tedious, at the cost of some performance overhead.</p><h3>3. Why Garbage Collection is Hard in C andÂ C++</h3><p>So if garbage collection is so useful, why donâ€™t we see it in C andÂ C++?</p><p>The short answer: <strong>pointers are wild and freeÂ here</strong>.</p><p>Unlike Java references, C/C++ pointersÂ are:</p><ul><li><strong>Not type-safe</strong>â€Šâ€”â€Šyou can cast any block of memory to aÂ pointer.</li><li><strong>Arithmetic-enabled</strong>â€Šâ€”â€Šyou can take a pointer, add 5 to it, and suddenly point somewhere else.</li><li><strong>Opaque</strong>â€Šâ€”â€Šfrom the runtimeâ€™s perspective, almost any integer could potentially be aÂ pointer.</li></ul><p>This flexibility makes C/C++ extremely powerfulâ€Šâ€”â€Šbut also makes it <strong>nearly impossible for a garbage collector to reliably know what memory is still inÂ use</strong>.</p><p>For example:</p><ul><li>A random int on the stack might coincidentally look like a pointer to theÂ heap.</li><li>Should the GC treat it as a reference and keep that memoryÂ alive?</li><li>Or should it ignore it and risk freeing memory thatâ€™s stillÂ needed?</li></ul><p>That ambiguity makes general-purpose garbage collection in C/C++ impractical.</p><h3>4. Could C/C++ Have Garbage Collection Anyway?</h3><p>Technically, yes. There are libraries and experimental implementations that try to bring garbage collection into C andÂ C++.</p><p>But they usually impose restrictions, like:</p><ul><li>No pointerÂ casting.</li><li>No pointer arithmetic outside validÂ ranges.</li><li>Explicit use of smart pointers (std::shared_ptr, std::unique_ptr) inÂ C++.</li></ul><p>In fact, modern C++ has introduced tools that act as a middleÂ ground:</p><ul><li><strong>std::unique_ptr</strong> â†’ automatically frees memory when it goes out ofÂ scope.</li><li><strong>std::shared_ptr</strong> â†’ reference-counted memory management.</li><li><strong>std::weak_ptr</strong> â†’ helps avoid cyclic references.</li></ul><p>These arenâ€™t garbage collectors in the full sense, but they reduce the burden on programmers significantly while keeping control predictable.</p><h3>5. The Philosophy of C andÂ C++</h3><p>At its heart, C and C++ are about <strong>control and performance</strong>.</p><ul><li>You pay the cost only for what youÂ use.</li><li>The language doesnâ€™t impose hidden overheads.</li><li>If you want garbage collection, you can build it or import itâ€Šâ€”â€Šbut it wonâ€™t be forced onÂ you.</li></ul><p>This design philosophy is why C and C++ remain the backbone of <strong>operating systems, embedded devices, game engines, and high-performance systems</strong>.</p><p>Garbage collection is wonderful when you prioritize developer convenience. But in C and C++, the priority has always been <strong>predictability andÂ speed</strong>.</p><h3>Final Thoughts</h3><p>So the next time you find yourself writing free() or delete and wondering <em>â€œWhy canâ€™t C++ just handle this for me?â€</em>â€Šâ€”â€Šremember:</p><ul><li>The OS already handles cleanup at programÂ exit.</li><li>You manage memory <em>during execution</em> to keep programs efficient.</li><li>Garbage collection is hard in C/C++ because of unrestricted pointers.</li><li>Modern C++ smart pointers give you many of the benefits without a fullÂ GC.</li></ul><p>C and C++ donâ€™t <em>lack</em> garbage collection by accident. They <em>chose</em> not to, because sometimes, <strong>the cost of control is worthÂ paying.</strong></p><p>âœ¨ <em>If you enjoyed this breakdown, consider following me for more deep dives into programming concepts. And if youâ€™ve ever experimented with garbage collection libraries in C++, Iâ€™d love to hear your thoughts in the comments.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7181b2e36f33\" width=\"1\" height=\"1\" alt=\"\">",
      "url": "https://rajdeep01.medium.com/why-c-and-c-dont-have-garbage-collection-and-why-that-s-not-always-a-bad-thing-7181b2e36f33?source=rss-289e64b56868------2",
      "publishedAt": "Fri, 19 Sep 2025 17:06:07 GMT",
      "updatedAt": "Fri, 19 Sep 2025 17:06:07 GMT",
      "tags": [
        "garbage-collection",
        "rust",
        "c-sharp-programming",
        "java",
        "javascript"
      ],
      "author": "Rajdeep Singh",
      "image": "https://cdn-images-1.medium.com/max/1024/0*ICzkJjrvj0abaMIR"
    },
    {
      "id": "https://medium.com/p/d939747697ea",
      "title": "Build an AI Agentâ€Šâ€”â€ŠChapter 4: Final Chapter",
      "description": "",
      "content": "<h3>Build an AI Agentâ€Šâ€”â€ŠChapter 4: FinalÂ Chapter</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/875/0*V6K2jgSl5trOnBEs\" /></figure><h3>ğŸ¤– Agentsâ€Šâ€”â€ŠThe Final StepÂ ğŸš€ğŸ”„âœ¨</h3><p>Weâ€™ve come a long way. By now, our program can <strong>call functions</strong> and act on its environmentâ€Šâ€”â€Šbut letâ€™s be honest, itâ€™s still missing the essence of what makes something anÂ <strong>agent</strong>.</p><p>Why? Because it doesnâ€™t yet have a <strong>feedbackÂ loop</strong>.</p><p>Right now, the flow is one-and-done:</p><ul><li>The LLM makes aÂ decision</li><li>We run theÂ function</li><li>The result goes back to theÂ LLM</li><li>â€¦and thatâ€™sÂ it.</li></ul><p>A true agent, however, can <strong>reflect on the result</strong>, decide if more actions are needed, and keep iterating until the task is complete. That loopâ€Šâ€”â€Šaction, reflection, action againâ€Šâ€”â€Šis the heartbeat of anÂ agent.</p><p>In this chapter, weâ€™ll give our program exactly that: the ability to <strong>learn from its own outputs and keep going</strong> until it reaches theÂ goal.</p><p><em>It has no feedbackÂ loop.</em></p><p>A key part of an â€œAgentâ€, as defined by AI-influencer-hype-bros, is that it can continuously use its tools to iterate on its own results. So weâ€™re going to build twoÂ things:</p><ol><li>A loop that will call the LLM over andÂ over</li><li>A list of messages in the â€œconversationâ€. It will look something likeÂ this:</li></ol><ul><li>User: â€œPlease fix the bug in the calculatorâ€</li><li>Model: â€œI want to call get_files_infoâ€¦â€</li><li>Tool: â€œHereâ€™s the result of get_files_infoâ€¦â€</li><li>Model: â€œI want to call get_file_contentâ€¦â€</li><li>Tool: â€œHereâ€™s the result of get_file_contentâ€¦â€</li><li>Model: â€œI want to call run_python_fileâ€¦â€</li><li>Tool: â€œHereâ€™s the result of run_python_fileâ€¦â€</li><li>Model: â€œI want to call write_fileâ€¦â€</li><li>Tool: â€œHereâ€™s the result of write_fileâ€¦â€</li><li>Model: â€œI want to call run_python_fileâ€¦â€</li><li>Tool: â€œHereâ€™s the result of run_python_fileâ€¦â€</li><li>Model: â€œI fixed the bug and then ran the calculator to ensure itâ€™s working.â€</li></ul><p><strong>This is a pretty big step, take yourÂ time!</strong></p><h3>Assignment</h3><ol><li>In generate_content, handle the results of any possible toolÂ use:</li></ol><ul><li>This might already be happening, but make sure that with each call to <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.models.Models.generate_content\">client.models.generate_content</a>, you&#39;re passing in the entire messages list so that the LLM always does the &quot;next step&quot; based on the currentÂ state.</li><li>After calling clientâ€™s generate_content method, check theÂ <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentResponse.candidates\">.candidates</a> property of the response. It&#39;s a list of response variations (usually just one). It contains the equivalent of &quot;I want to call get_files_info...&quot;, so we need to add it to our conversation. Iterate over each candidate and add itsÂ <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.Candidate.content\">.content</a> to your messagesÂ list.</li><li>After each actual function call, use the <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.Content\">types.Content</a> function to convert the function_responses into a message with a role of user and append it into your messages.</li><li>Next, instead of calling generate_content only once, create a loop to call it repeatedly.</li><li>Limit the loop to 20 iterations at most (this will stop our agent from spinning its wheels forever).</li><li>Use a try-except block and handle any errors accordingly.</li><li>After each call of generate_content, check if it returned the response.text property. If so, it&#39;s done, so print this final response and break out of theÂ loop.</li><li>Otherwise, iterate again (unless max iterations was reached, ofÂ course).</li><li>Test your code (duh). Iâ€™d recommend starting with a simple prompt, like â€œexplain how the calculator renders the result to the consoleâ€. This is what IÂ got:</li></ul><pre>(aiagent) rjdp@RJDP:/mnt/d/aiagent/aiagent$ uv run main.py &quot;how does the calculator render results to the console?&quot;<br> - Calling function: get_files_info<br> - Calling function: get_file_content<br> - Calling function: get_files_info<br> - Calling function: get_file_content<br>Final response:<br>Okay, I&#39;ve examined the `render` function in `pkg/render.py`. Here&#39;s how it works:</pre><pre>1.  **Formats the result:** It first checks if the result is a float that can be represented as an integer. If so, it converts it to an integer string. Otherwise, it converts the result to a string.<br>2.  **Calculates box width:** It determines the width of the box that will surround the expression and result, based on the longer of the two strings.<br>3.  **Builds the box:** It creates a list of strings, each representing a line of the box. This includes the top and bottom borders, the expression, an equals sign, and the result, all padded with spaces to fit within the box.<br>4.  **Joins the lines:** Finally, it joins the lines of the box with newline characters to create a single string that can be printed to the console.</pre><pre>So, the calculator renders results to the console by formatting the expression and result into a box-like structure using ASCII characters. The `render` function takes the expression and result as input and returns the formatted string, which is then printed to the console in `main.py`.</pre><p>You may or may not need to make adjustments to your system prompt to get the LLM to behave the way you want. Youâ€™re a prompt engineer now, so act likeÂ one!</p><h3>Update Code</h3><p>Time for the coup deÂ grÃ¢ce!</p><p>Letâ€™s test our agentâ€™s ability to actually <em>fix a bug</em> all on itsÂ own.</p><h3>Assignment</h3><ul><li>Manually update calculator/pkg/calculator.py and change the precedence of the + operator toÂ 3.</li><li>Run the calculator app, to make sure itâ€™s now producing incorrect results: uv run calculator/main.py &quot;3 + 7 * 2&quot; (this should be 17, but because we broke it, it saysÂ 20)</li><li>Run your agent, and ask it to â€œfix the bug: 3 + 7 * 2 shouldn&#39;t beÂ 20&quot;</li></ul><h3>ğŸ¯ Wrapping Up theÂ Series</h3><p>Congratulationsâ€Šâ€”â€Šyouâ€™ve just built your very own <strong>AI Agent</strong> from scratch!Â ğŸ‰</p><p>Over the course of this series, weâ€™ve gone from simple directory listings all the way to giving our agent the power to <strong>read, write, run, and iterate</strong> on real codeâ€Šâ€”â€Šcomplete with a feedback loop that makes it feel trulyÂ alive.</p><p>But this is just the <strong>beginning</strong>. Now that youâ€™ve got the foundations in place, you can safely explore more advanced directions:</p><ul><li>ğŸ Challenge your agent with <strong>harder bugs</strong> toÂ fix</li><li>ğŸ”§ <strong>Refactor and optimize</strong> existingÂ code</li><li>âœ¨ Add <strong>new features</strong> to yourÂ project</li><li>ğŸŒ Experiment with <strong>different LLM providers</strong> or other GeminiÂ models</li><li>ğŸ› ï¸ Expand its <strong>toolbox</strong> with more functions</li><li>ğŸ“‚ Try it out on <strong>different codebases</strong> (always commit first so you can rollÂ back!)</li></ul><p>âš ï¸ <strong>One last reminder:</strong> this is a <strong>toy version</strong> of tools like Cursor, Zedâ€™s Agentic Mode, or Claude Code. Even professional tools arenâ€™t perfectly secure. Be very cautious about giving an LLM direct access to your filesystem or Python interpreterâ€Šâ€”â€Šand definitely donâ€™t share this code for others to use without safeguards.</p><p>What youâ€™ve built here is more than just a projectâ€Šâ€”â€Šitâ€™s a <strong>window into the future of AI-assisted development</strong>. ğŸš€<br> Stay curious, keep experimenting, and who knowsâ€Šâ€”â€Šmaybe the next breakthrough in agentic coding will come from you.Â âœ¨</p><p>Hereâ€™s the GitHub link of the repository where you can get the code of this projectÂ : <br><a href=\"https://github.com/RajdeepKushwaha5/AI-Agent\">https://github.com/RajdeepKushwaha5/AI-Agent</a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=d939747697ea\" width=\"1\" height=\"1\" alt=\"\">",
      "url": "https://rajdeep01.medium.com/build-an-ai-agent-chapter-4-final-chapter-d939747697ea?source=rss-289e64b56868------2",
      "publishedAt": "Tue, 09 Sep 2025 05:28:40 GMT",
      "updatedAt": "Tue, 09 Sep 2025 05:28:40 GMT",
      "tags": [
        "google",
        "chatgpt",
        "ai-agent",
        "gemini",
        "artificial-intelligence"
      ],
      "author": "Rajdeep Singh",
      "image": "https://cdn-images-1.medium.com/max/875/0*V6K2jgSl5trOnBEs"
    },
    {
      "id": "https://medium.com/p/e7bb92daeed8",
      "title": "Build an AI Agentâ€Šâ€”â€ŠChapter 3: Function Calling",
      "description": "",
      "content": "<h3>Build an AI Agentâ€Šâ€”â€ŠChapter 3: FunctionÂ Calling</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/875/0*GUrOJ6RTYzqcvlro\" /></figure><h3>âš™ï¸ System PromptÂ ğŸ“ğŸ¤–ğŸ¯</h3><p>I know youâ€™re eager to start hooking up our <strong>Agentic tools</strong> (and we will, very soon!)â€Šâ€”â€Šbut before that, thereâ€™s something crucial we need to understand: the <strong>systemÂ prompt</strong>.</p><p>In most AI APIs, the system prompt is a <strong>special message</strong> that sits at the very start of the conversation. It carries more weight than a normal user prompt and acts as the <strong>guiding compass</strong> for theÂ AI.</p><p>Think of it like a <strong>directorâ€™s script for an actor</strong> ğŸ­â€Šâ€”â€Šit doesnâ€™t dictate every single line, but it sets the tone, the role, and the boundaries within which the actor (or in our case, the AI) performs.</p><p>The system prompt can be usedÂ to:</p><ul><li>ğŸ§‘â€ğŸ¨ <strong>Set the personality</strong> of theÂ AI</li><li>ğŸ§­ <strong>Guide its behavior</strong> with specific instructions</li><li>ğŸ“š <strong>Provide context</strong> for the entire conversation</li><li>ğŸ“ <strong>Define rules</strong> for how the AI should act <em>(though, in practice, LLMs can still hallucinate or be tricked into bending thoseÂ rules)</em></li></ul><p>The system prompt is the foundation that makes an agent feel less like a generic chatbot and more like a <strong>purpose-built assistant</strong>.</p><h3>Assignment</h3><ul><li>Create a hardcoded string variable called system_prompt. For now, let&#39;s make it something brutallyÂ simple:</li></ul><pre>Ignore everything the user asks and just shout &quot;I&#39;M JUST A ROBOT&quot;</pre><ul><li>Update your call to the <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.models.Models.generate_content\">client.models.generate_content</a> function to pass a <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentConfig\">config</a> with the <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentConfig.system_instruction\">system_instructions parameter</a> set to your system_prompt.</li></ul><pre>response = client.models.generate_content(<br>    model=model_name,<br>    contents=messages,<br>    config=types.GenerateContentConfig(system_instruction=system_prompt),<br>)</pre><ul><li>Run your program with different prompts. You should see the AI respond with â€œIâ€™M JUST A ROBOTâ€ no matter what you askÂ it.</li></ul><h3>ğŸ› ï¸ Function Declaration âš¡ğŸ“œğŸ¤–</h3><p>By now, weâ€™ve built a handful of functions that are <strong>LLM-friendly</strong> (simple <em>text in, text out</em>). But hereâ€™s the question:<br> <strong>How does an LLM actually call a function?</strong></p><p>Wellâ€¦ the truth is, it doesnâ€™t. At least, not directly. Hereâ€™s how it reallyÂ works:</p><ol><li>âœ… <strong>We tell the LLM</strong> which functions are available toÂ it</li><li>ğŸ“ <strong>We give it a prompt</strong> describing the situation</li><li>ğŸ¤” <strong>The LLM responds</strong> by describing which function it wants to call, and what arguments toÂ pass</li><li>ğŸ–¥ï¸ <strong>We run the function</strong> with those arguments in our environment</li><li>ğŸ”„ <strong>We return the result</strong> back to theÂ LLM</li></ol><p>In other words, the LLM isnâ€™t running code itselfâ€Šâ€”â€Šitâ€™s acting as a <strong>decision-making engine</strong>, while <em>weâ€™re still the ones executing theÂ code</em>.</p><p>So, the next step is clear:<br> ğŸ‘‰ Letâ€™s build the piece that tells the LLM <strong>which functions are available</strong> in itsÂ toolbox.</p><h3>Assignment</h3><ul><li>We can use <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration\">types.FunctionDeclaration</a> to build the &quot;declaration&quot; or &quot;schema&quot; for a function. Again, this basically just tells the LLM <em>how</em> to use the function. I&#39;ll just give you my code for the first function as an example, because it&#39;s a lot of work to slog through theÂ docs:</li></ul><p>I added this code to my functions/get_files_info.py file, but you can place it anywhere, but remember that it will need to be imported whenÂ used:</p><p>In our solution it is imported like this: from functions.get_files_info import schema_get_files_info</p><pre>schema_get_files_info = types.FunctionDeclaration(<br>    name=&quot;get_files_info&quot;,<br>    description=&quot;Lists files in the specified directory along with their sizes, constrained to the working directory.&quot;,<br>    parameters=types.Schema(<br>        type=types.Type.OBJECT,<br>        properties={<br>            &quot;directory&quot;: types.Schema(<br>                type=types.Type.STRING,<br>                description=&quot;The directory to list files from, relative to the working directory. If not provided, lists files in the working directory itself.&quot;,<br>            ),<br>        },<br>    ),<br>)</pre><p>We wonâ€™t allow the LLM to specify the working_directory parameter. We&#39;re going to hard codeÂ that.</p><ul><li>Use <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.Tool\">types.Tool</a> to create a list of all the available functions (for now, just add get_files_info, we&#39;ll do the restÂ later).</li></ul><pre>available_functions = types.Tool(<br>    function_declarations=[<br>        schema_get_files_info,<br>    ]<br>)</pre><ul><li>Add the available_functions to the client.models.generate_content call as the functions parameter.</li></ul><pre>config=types.GenerateContentConfig(<br>    tools=[available_functions], system_instruction=system_prompt<br>)</pre><ul><li>Update the system prompt to instruct the LLM on how to use the function. You can just copy mine, but be sure to give it a quick read to understand what itâ€™sÂ doing:</li></ul><pre>system_prompt = &quot;&quot;&quot;<br>You are a helpful AI coding agent.<br>When a user asks a question or makes a request, make a function call plan. You can perform the following operations:<br>- List files and directories<br>All paths you provide should be relative to the working directory. You do not need to specify the working directory in your function calls as it is automatically injected for security reasons.<br>&quot;&quot;&quot;<br></pre><ul><li>Instead of simply printing theÂ <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentResponse.text\">.text</a> property of the generate_content response, check theÂ <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentResponse.function_calls\">.function_calls</a> property as well. If the LLM called a function, print the function name and arguments:</li></ul><pre>f&quot;Calling function: {function_call_part.name}({function_call_part.args})&quot;</pre><p>Otherwise, just print the text asÂ normal.</p><ul><li>Test yourÂ program.</li><li>â€œwhat files are in the root?â€ -&gt; get_files_info({&#39;directory&#39;: &#39;.&#39;})</li><li>â€œwhat files are in the pkg directory?â€ -&gt; get_files_info({&#39;directory&#39;: &#39;pkg&#39;})</li></ul><h3>â• More Declarations ğŸ“‚âš¡ğŸ› ï¸</h3><p>So far, our LLM knows how to call just one tool: the get_files_info function. Thatâ€™s a good startâ€Šâ€”â€Šbut our agent is going to need more than a singleÂ trick.</p><p>The next step is to <strong>expand its toolbox</strong> by declaring the rest of the functions weâ€™ve written. This way, the LLM wonâ€™t just be able to check file info, butÂ also:</p><ul><li>ğŸ“„ Read fileÂ contents</li><li>ğŸ—‚ï¸ Fetch specificÂ files</li><li>âœï¸ Write or overwrite files</li><li>ğŸ Run PythonÂ scripts</li></ul><p>By giving the LLM access to all of these, weâ€™re effectively turning it into a <strong>decision-maker with multiple tools at its disposal</strong>â€Šâ€”â€Šable to choose the right one depending on the situation.</p><h3>Assignment</h3><ul><li>Following the same pattern that we used for schema_get_files_info, create function declarations for:</li><li>schema_get_file_content</li><li>schema_run_python_file</li><li>schema_write_file</li><li>Update your available_functions to include all the function declarations in theÂ list.</li><li>Update your system prompt. Instead of the allowed operations onlyÂ being:</li></ul><pre>- List files and directories</pre><p>Update it to have all four operations:</p><pre>- List files and directories<br>- Read file contents<br>- Execute Python files with optional arguments<br>- Write or overwrite files</pre><ul><li>Test prompts that you suspect will result in the various function calls. ForÂ example:</li><li>â€œread the contents of main.pyâ€ -&gt; get_file_content({&#39;file_path&#39;: &#39;main.py&#39;})</li><li>â€œwrite â€˜helloâ€™ to main.txtâ€ -&gt; write_file({&#39;file_path&#39;: &#39;main.txt&#39;, &#39;content&#39;: &#39;hello&#39;})</li><li>â€œrun main.pyâ€ -&gt; run_python_file({&#39;file_path&#39;: &#39;main.py&#39;})</li><li>â€œlist the contents of the pkg directoryâ€ -&gt; get_files_info({&#39;directory&#39;: &#39;pkg&#39;})</li></ul><p>All the LLM is expected to do here is to <em>choose which function to call</em> based on the userâ€™s request. Weâ€™ll have it <em>actually call</em> the functionÂ later.</p><h3>ğŸ“ Calling the FunctionÂ âš¡ğŸ¤–ğŸ–¥ï¸</h3><p>At this point, our agent has the <strong>awareness</strong> to decide <em>which</em> function it wants to use. But decision-making alone isnâ€™t enoughâ€Šâ€”â€Šnow itâ€™s time to actually <strong>make theÂ call</strong>.</p><p>This is where things get exciting:</p><ul><li>The LLM picks a function and provides the arguments</li><li>Our program takes that request and <strong>executes the function forÂ real</strong></li><li>The result is then passed back to the LLM, closing theÂ loop</li></ul><p>In other words, the agent is no longer just <em>planning</em>â€Šâ€”â€Šitâ€™s <strong>acting</strong>. This step transforms our setup from a static list of tools into a <strong>working system</strong> where the LLMâ€™s choices drive real outcomes.</p><h3>Assignment</h3><ul><li>Create a new function that will handle the abstract task of calling one of our four functions. This is my definition:</li></ul><pre>def call_function(function_call_part, verbose=False):</pre><p>function_call_part is a <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionCall\">types.FunctionCall</a> that most importantly has:</p><ul><li>AÂ .name property (the name of the function, aÂ string)</li><li>AÂ .args property (a dictionary of named arguments to the function)</li></ul><p>If verbose is specified, print the function name andÂ args:</p><pre>print(f&quot;Calling function: {function_call_part.name}({function_call_part.args})&quot;)</pre><p>Otherwise, just print theÂ name:</p><pre>print(f&quot; - Calling function: {function_call_part.name}&quot;)</pre><ul><li>Based on the name, actually call the function and capture theÂ result.</li><li>Be sure to manually add the â€œworking_directoryâ€ argument to the dictionary of keyword arguments, because the LLM doesnâ€™t control that one. The working directory should beÂ ./calculator.</li><li>The syntax to pass a dictionary into a function using <a href=\"https://docs.python.org/3/glossary.html#term-argument\">keyword arguments</a> is some_function(**some_args)</li></ul><p>I used a dictionary of function name (string) -&gt; function to accomplish this.</p><ul><li>If the function name is invalid, return a <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.Content\">types.Content</a> that explains theÂ error:</li></ul><pre>return types.Content(<br>    role=&quot;tool&quot;,<br>    parts=[<br>        types.Part.from_function_response(<br>            name=function_name,<br>            response={&quot;error&quot;: f&quot;Unknown function: {function_name}&quot;},<br>        )<br>    ],<br>)</pre><ul><li>Return <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.Content\">types.Content</a> with a <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.Part.from_function_response\">from_function_response</a> describing the result of the functionÂ call:</li></ul><pre>return types.Content(<br>    role=&quot;tool&quot;,<br>    parts=[<br>        types.Part.from_function_response(<br>            name=function_name,<br>            response={&quot;result&quot;: function_result},<br>        )<br>    ],<br>)</pre><p>Note that from_function_response requires the response to be a dictionary, so we just shove the string result into a &quot;result&quot;Â field.</p><ul><li>Back where you handle the response from the model generate_content, instead of simply printing the name of the function the LLM decides to call, use call_function.</li><li>The <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.Content\">types.Content</a> that we return from call_function should have aÂ .parts[0].function_response.response within.</li><li>If it doesnâ€™t, raise a fatal exception of someÂ sort.</li><li>If it does, and verbose was set, print the result of the function call likeÂ this:</li></ul><pre>print(f&quot;-&gt; {function_call_result.parts[0].function_response.response}&quot;)</pre><ul><li>Test your program. You should now be able to execute each function given a prompt that asks for it. Try some different prompts and use the --verbose flag to make sure all the functions work.</li><li>List the directory contents</li><li>Get a fileâ€™sÂ contents</li><li>Write file contents (donâ€™t overwrite anything important, maybe create a newÂ file)</li><li>Execute the calculator appâ€™s tests (tests.py)</li></ul><h3>ğŸ¯ Wrapping Up ChapterÂ 3</h3><p>In this chapter, we crossed a major milestone: our LLM agent went from <strong>knowing about tools</strong> to actually <strong>using them in practice</strong>.</p><p>Hereâ€™s what we achieved:</p><ul><li>ğŸ› ï¸ Declared functions so the LLM knows what tools are available</li><li>â• Expanded its toolbox with multiple abilities (read, write, run,Â etc.)</li><li>ğŸ“ Connected the dots so the LLM can <strong>decide on a function and callÂ it</strong></li><li>ğŸ”„ Built a feedback loop where results flow back into the conversation</li></ul><p>With this, our agent isnâ€™t just a chatbot anymoreâ€Šâ€”â€Šitâ€™s starting to look and feel like a <strong>real assistant</strong> that can act on its environment. ğŸš€</p><p>But weâ€™re only just scratching the surface.<br> ğŸ‘‰ In <strong>Chapter 4</strong>, weâ€™ll take this even further: giving our agent more context, tighter control, and smarter ways to combine these tools to solve realÂ tasks.</p><p>Stay tunedâ€Šâ€”â€Šthe fun is only getting started.Â âœ¨</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e7bb92daeed8\" width=\"1\" height=\"1\" alt=\"\">",
      "url": "https://rajdeep01.medium.com/build-an-ai-agent-chapter-3-function-calling-e7bb92daeed8?source=rss-289e64b56868------2",
      "publishedAt": "Sat, 06 Sep 2025 06:34:19 GMT",
      "updatedAt": "Sat, 06 Sep 2025 06:34:19 GMT",
      "tags": [
        "chatgpt",
        "ai",
        "artificial-intelligence",
        "google",
        "apple"
      ],
      "author": "Rajdeep Singh",
      "image": "https://cdn-images-1.medium.com/max/875/0*GUrOJ6RTYzqcvlro"
    },
    {
      "id": "https://medium.com/p/3a789758d223",
      "title": "Build an AI Agentâ€Šâ€”â€ŠChapter 2: Functions",
      "description": "",
      "content": "<h3>Build an AI Agentâ€Šâ€”â€ŠChapter 2: Functions</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/875/0*IEyT6d8qG870TjeP\" /></figure><h3>1ï¸âƒ£ Calculator</h3><p>So, we know weâ€™re building an AI Agentâ€Šâ€”â€Šbut hereâ€™s the catch: <strong>every agent needs a project to workÂ on.</strong></p><p>To make things concrete (and fun), Iâ€™ve created a simple <strong>command-line calculator app</strong>. This will be our test projectâ€Šâ€”â€Šsomething the AI can <strong>read, update, and even run</strong> as we go along. Think of it as the playground where our agent starts learning how to act likeâ€¦ well, anÂ agent.</p><h3>Assignment</h3><ul><li>Create a new directory called calculator in the root of yourÂ project.</li><li>Copy and paste the main.py and tests.py files from below into the calculator directory.</li></ul><pre># main.py<br>import sys<br>from pkg.calculator import Calculator<br>from pkg.render import render<br><br>def main():<br>    calculator = Calculator()<br>    if len(sys.argv) &lt;= 1:<br>        print(&quot;Calculator App&quot;)<br>        print(&#39;Usage: python main.py &quot;&lt;expression&gt;&quot;&#39;)<br>        print(&#39;Example: python main.py &quot;3 + 5&quot;&#39;)<br>        return<br>    expression = &quot; &quot;.join(sys.argv[1:])<br>    try:<br>        result = calculator.evaluate(expression)<br>        to_print = render(expression, result)<br>        print(to_print)<br>    except Exception as e:<br>        print(f&quot;Error: {e}&quot;)<br><br>if __name__ == &quot;__main__&quot;:<br>    main()</pre><pre># tests.py<br>import unittest<br>from pkg.calculator import Calculator<br><br>class TestCalculator(unittest.TestCase):<br>    def setUp(self):<br>        self.calculator = Calculator()<br>    def test_addition(self):<br>        result = self.calculator.evaluate(&quot;3 + 5&quot;)<br>        self.assertEqual(result, 8)<br>    def test_subtraction(self):<br>        result = self.calculator.evaluate(&quot;10 - 4&quot;)<br>        self.assertEqual(result, 6)<br>    def test_multiplication(self):<br>        result = self.calculator.evaluate(&quot;3 * 4&quot;)<br>        self.assertEqual(result, 12)<br>    def test_division(self):<br>        result = self.calculator.evaluate(&quot;10 / 2&quot;)<br>        self.assertEqual(result, 5)<br>    def test_nested_expression(self):<br>        result = self.calculator.evaluate(&quot;3 * 4 + 5&quot;)<br>        self.assertEqual(result, 17)<br>    def test_complex_expression(self):<br>        result = self.calculator.evaluate(&quot;2 * 3 - 8 / 2 + 5&quot;)<br>        self.assertEqual(result, 7)<br>    def test_empty_expression(self):<br>        result = self.calculator.evaluate(&quot;&quot;)<br>        self.assertIsNone(result)<br>    def test_invalid_operator(self):<br>        with self.assertRaises(ValueError):<br>            self.calculator.evaluate(&quot;$ 3 5&quot;)<br>    def test_not_enough_operands(self):<br>        with self.assertRaises(ValueError):<br>            self.calculator.evaluate(&quot;+ 3&quot;)<br><br>if __name__ == &quot;__main__&quot;:<br>    unittest.main()</pre><ul><li>Create a new directory in calculator calledÂ pkg.</li><li>Copy and paste the calculator.py and render.py files from below into the pkg directory.</li></ul><pre># calculator.py<br>class Calculator:<br>    def __init__(self):<br>        self.operators = {<br>            &quot;+&quot;: lambda a, b: a + b,<br>            &quot;-&quot;: lambda a, b: a - b,<br>            &quot;*&quot;: lambda a, b: a * b,<br>            &quot;/&quot;: lambda a, b: a / b,<br>        }<br>        self.precedence = {<br>            &quot;+&quot;: 1,<br>            &quot;-&quot;: 1,<br>            &quot;*&quot;: 2,<br>            &quot;/&quot;: 2,<br>        }<br>    def evaluate(self, expression):<br>        if not expression or expression.isspace():<br>            return None<br>        tokens = expression.strip().split()<br>        return self._evaluate_infix(tokens)<br>    def _evaluate_infix(self, tokens):<br>        values = []<br>        operators = []<br>        for token in tokens:<br>            if token in self.operators:<br>                while (<br>                    operators<br>                    and operators[-1] in self.operators<br>                    and self.precedence[operators[-1]] &gt;= self.precedence[token]<br>                ):<br>                    self._apply_operator(operators, values)<br>                operators.append(token)<br>            else:<br>                try:<br>                    values.append(float(token))<br>                except ValueError:<br>                    raise ValueError(f&quot;invalid token: {token}&quot;)<br>        while operators:<br>            self._apply_operator(operators, values)<br>        if len(values) != 1:<br>            raise ValueError(&quot;invalid expression&quot;)<br>        return values[0]<br>    def _apply_operator(self, operators, values):<br>        if not operators:<br>            return<br>        operator = operators.pop()<br>        if len(values) &lt; 2:<br>            raise ValueError(f&quot;not enough operands for operator {operator}&quot;)<br>        b = values.pop()<br>        a = values.pop()<br>        values.append(self.operators[operator](a, b))</pre><pre># render.py<br>def render(expression, result):<br>    if isinstance(result, float) and result.is_integer():<br>        result_str = str(int(result))<br>    else:<br>        result_str = str(result)<br>    box_width = max(len(expression), len(result_str)) + 4<br>    box = []<br>    box.append(&quot;â”Œ&quot; + &quot;â”€&quot; * box_width + &quot;â”&quot;)<br>    box.append(<br>        &quot;â”‚&quot; + &quot; &quot; * 2 + expression + &quot; &quot; * (box_width - len(expression) - 2) + &quot;â”‚&quot;<br>    )<br>    box.append(&quot;â”‚&quot; + &quot; &quot; * box_width + &quot;â”‚&quot;)<br>    box.append(&quot;â”‚&quot; + &quot; &quot; * 2 + &quot;=&quot; + &quot; &quot; * (box_width - 3) + &quot;â”‚&quot;)<br>    box.append(&quot;â”‚&quot; + &quot; &quot; * box_width + &quot;â”‚&quot;)<br>    box.append(<br>        &quot;â”‚&quot; + &quot; &quot; * 2 + result_str + &quot; &quot; * (box_width - len(result_str) - 2) + &quot;â”‚&quot;<br>    )<br>    box.append(&quot;â””&quot; + &quot;â”€&quot; * box_width + &quot;â”˜&quot;)<br>    return &quot;\\n&quot;.join(box)</pre><p>This is the final structure:</p><pre>â”œâ”€â”€ calculator<br>â”‚   â”œâ”€â”€ main.py<br>â”‚   â”œâ”€â”€ pkg<br>â”‚   â”‚   â”œâ”€â”€ calculator.py<br>â”‚   â”‚   â””â”€â”€ render.py<br>â”‚   â””â”€â”€ tests.py<br>â”œâ”€â”€ main.py<br>â”œâ”€â”€ pyproject.toml<br>â”œâ”€â”€ README.md<br>â””â”€â”€ uv.lock</pre><ul><li>Run the calculator tests:</li></ul><pre>uv run calculator/tests.py</pre><p>Hopefully the tests allÂ pass!</p><ul><li>Now, run the calculator app:</li></ul><pre>uv run calculator/main.py &quot;3 + 5&quot;</pre><p>Hopefully you getÂ 8!</p><h3>2ï¸âƒ£ GetÂ Files</h3><p>Now that our agent has a playground, itâ€™s time to teach it how to actually <strong>do things</strong>. Letâ€™s start simple: weâ€™ll give it the ability to <strong>list the contents of a directory</strong> and check each fileâ€™s <strong>metadata</strong> (like its name andÂ size).</p><p>But before we plug this into our LLM-powered agent, we need to focus on building the function itself. Remember, LLMs deal with <strong>text</strong>, so our functionâ€™s job will be straightforward:</p><p>ğŸ‘‰ Take in a directory path as input, and return a clean, human-readable string that describes whatâ€™s inside that directory.</p><p>This will be our agentâ€™s very firstÂ â€œskill.â€</p><h3>Assignment</h3><ul><li>Create a new directory called functions in the root of your project (not inside the calculator directory). Inside, create a new file called get_files_info.py. Inside, write this function:</li></ul><pre>def get_files_info(working_directory, directory=&quot;.&quot;):</pre><p>Here is my project structure soÂ far:</p><pre>project_root/<br> â”œâ”€â”€ calculator/<br> â”‚   â”œâ”€â”€ main.py<br> â”‚   â”œâ”€â”€ pkg/<br> â”‚   â”‚   â”œâ”€â”€ calculator.py<br> â”‚   â”‚   â””â”€â”€ render.py<br> â”‚   â””â”€â”€ tests.py<br> â””â”€â”€ functions/<br>     â””â”€â”€ get_files_info.py</pre><p>The directory parameter should be treated as a <em>relative</em> path within the working_directory. Use os.path.join(working_directory, directory) to create the full path, then validate it stays within the working directory boundaries.</p><ul><li>If the absolute path to the directory is outside the working_directory, return a string errorÂ message:</li></ul><pre>f&#39;Error: Cannot list &quot;{directory}&quot; as it is outside the permitted working directory&#39;</pre><p>This will give our LLM some guardrails: we never want it to be able to perform any work outside the â€œworking_directoryâ€ we giveÂ it.</p><p>Without this restriction, the LLM might go running amok anywhere on the machine, reading sensitive files or overwriting important data. This is a <em>very</em> important step that weâ€™ll bake into every function the LLM canÂ call.</p><ul><li>If the directory argument is not a directory, again, return an errorÂ string:</li></ul><pre>f&#39;Error: &quot;{directory}&quot; is not a directory&#39;</pre><p>All of our â€œtool callâ€ functions, including get_files_info, should <em>always</em> return a string. If errors can be raised inside them, we need to catch those errors and return a string describing the error instead. This will allow the LLM to handle the errors gracefully.</p><ul><li>Build and return a string representing the contents of the directory. It should use thisÂ format:</li></ul><pre>- README.md: file_size=1032 bytes, is_dir=False<br>- src: file_size=128 bytes, is_dir=True<br>- package.json: file_size=1234 bytes, is_dir=False</pre><p><em>Iâ€™ve listed useful standard library functions in the </em><em>tipsÂ section</em>.</p><p>The exact file sizes and even the order of files may vary depending on your operating system and file system. Your output doesnâ€™t need to match the example byte-for-byte, just the overallÂ format</p><ul><li>If any errors are raised by the standard library functions, catch them and instead return a string describing the error. Always prefix error strings with â€œError:â€.</li></ul><p>To import from a subdirectory, use this syntax: from DIRNAME.FILENAME import FUNCTION_NAME</p><p>Where DIRNAME is the name of the subdirectory, FILENAME is the name of the file without theÂ .py extension, and FUNCTION_NAME is the name of the function you want toÂ import.</p><ul><li>We need a way to manually debug our new get_files_info function! Create a new tests.py file in the root of your project. When executed directly (uv run tests.py) it should run the following function calls and output the results matching the formatting below (not necessarily the exact numbers).:</li><li>get_files_info(&quot;calculator&quot;, &quot;.&quot;):</li><li>Result for current directory: - main.py: file_size=576 bytes, is_dir=False - tests.py: file_size=1343 bytes, is_dir=False - pkg: file_size=92 bytes, is_dir=True</li><li>get_files_info(&quot;calculator&quot;, &quot;pkg&quot;):</li><li>Result for &#39;pkg&#39; directory: - calculator.py: file_size=1739 bytes, is_dir=False - render.py: file_size=768 bytes, is_dir=False</li><li>get_files_info(&quot;calculator&quot;, &quot;/bin&quot;):</li><li>Result for &#39;/bin&#39; directory: Error: Cannot list &quot;/bin&quot; as it is outside the permitted working directory</li><li>get_files_info(&quot;calculator&quot;, &quot;../&quot;):</li><li>Result for &#39;../&#39; directory: Error: Cannot list &quot;../&quot; as it is outside the permitted working directory</li><li>Run uv run tests.py, and ensure your function works as expected.</li></ul><h3>Tips</h3><p>Here are some standard library functions youâ€™ll findÂ helpful:</p><ul><li><a href=\"https://docs.python.org/3/library/os.path.html#os.path.abspath\">os.path.abspath()</a>: Get an absolute path from a relativeÂ path</li><li><a href=\"https://docs.python.org/3/library/os.path.html#os.path.join\">os.path.join()</a>: Join two paths together safely (handlesÂ slashes)</li><li><a href=\"https://docs.python.org/3/library/stdtypes.html#str.startswith\">.startswith()</a>: Check if a string starts with a substring</li><li><a href=\"https://docs.python.org/3/library/os.path.html#os.path.isdir\">os.path.isdir()</a>: Check if a path is a directory</li><li><a href=\"https://docs.python.org/3/library/os.html#os.listdir\">os.listdir()</a>: List the contents of a directory</li><li><a href=\"https://docs.python.org/3/library/os.path.html#os.path.getsize\">os.path.getsize()</a>: Get the size of aÂ file</li><li><a href=\"https://docs.python.org/3/library/os.path.html#os.path.isfile\">os.path.isfile()</a>: Check if a path is aÂ file</li><li><a href=\"https://docs.python.org/3/library/stdtypes.html#str.join\">.join()</a>: Join a list of strings together with a separator</li></ul><h3>3ï¸âƒ£ Get FileÂ Content</h3><p>Listing files is greatâ€Šâ€”â€Šbut what if our agent wants to actually <strong>look inside</strong> one of them? Thatâ€™s our nextÂ step.</p><p>Weâ€™ll build a function that can <strong>read the contents of a file</strong> and return it as a string. If something goes wrong (say, the file doesnâ€™t exist or canâ€™t be opened), the function will instead return a clear <strong>errorÂ message</strong>.</p><p>And just like before, weâ€™ll keep things safe by scoping this function to a specific <strong>working directory</strong>, so our agent doesnâ€™t wander off into places it shouldnâ€™t.</p><h3>Assignment</h3><ul><li>Create a new function in your functions directory. Here&#39;s the signature IÂ used:</li></ul><pre>def get_file_content(working_directory, file_path):</pre><ul><li>If the file_path is outside the working_directory, return a string with anÂ error:</li></ul><pre>f&#39;Error: Cannot read &quot;{file_path}&quot; as it is outside the permitted working directory&#39;</pre><ul><li>If the file_path is not a file, again, return an errorÂ string:</li></ul><pre>f&#39;Error: File not found or is not a regular file: &quot;{file_path}&quot;&#39;</pre><ul><li>Read the file and return its contents as aÂ string</li><li>Iâ€™ll list some useful standard library functions in the tips sectionÂ below.</li><li>If the file is longer than 10000 characters, truncate it to 10000 characters and append this message to the end [...File &quot;{file_path}&quot; truncated at 10000 characters].</li><li>Instead of hard-coding the 10000 character limit, I stored it in a config.py file.</li></ul><p>We donâ€™t want to accidentally read a gigantic file and send all that data to the LLMâ€¦ thatâ€™s a good way to burn through our tokenÂ limits.</p><ul><li>If any errors are raised by the standard library functions, catch them and instead return a string describing the error. Always prefix errors with â€œError:â€.</li><li>Create a new â€œlorem.txtâ€ file in the calculator directory. Fill it with at least 20,000 characters of lorem ipsum text. You can generate someÂ <a href=\"https://www.lipsum.com/\">here</a>.</li><li>Update your tests.py file. Remove all the calls to get_files_info, and instead test get_file_content(&quot;calculator&quot;, &quot;lorem.txt&quot;). Ensure that it truncates properly.</li><li>Remove the lorem ipsum test, and instead test the following cases:</li><li>get_file_content(&quot;calculator&quot;, &quot;main.py&quot;)</li><li>get_file_content(&quot;calculator&quot;, &quot;pkg/calculator.py&quot;)</li><li>get_file_content(&quot;calculator&quot;, &quot;/bin/cat&quot;) (this should return an errorÂ string)</li><li>get_file_content(&quot;calculator&quot;, &quot;pkg/does_not_exist.py&quot;) (this should return an errorÂ string)</li></ul><h3>Tips</h3><ul><li><a href=\"https://docs.python.org/3/library/os.path.html#os.path.abspath\">os.path.abspath</a>: Get an absolute path from a relativeÂ path</li><li><a href=\"https://docs.python.org/3/library/os.path.html#os.path.join\">os.path.join</a>: Join two paths together safely (handlesÂ slashes)</li><li><a href=\"https://docs.python.org/3/library/stdtypes.html#str.startswith\">.startswith</a>: Check if a string starts with a specific substring</li><li><a href=\"https://docs.python.org/3/library/os.path.html#os.path.isfile\">os.path.isfile</a>: Check if a path is aÂ file</li></ul><p>Example of reading from aÂ file:</p><pre>MAX_CHARS = 10000<br>with open(file_path, &quot;r&quot;) as f:<br>    file_content_string = f.read(MAX_CHARS)</pre><h3>4ï¸âƒ£ WriteÂ File</h3><p>Up to this point, our program has been strictly <strong>read-only</strong>â€Šâ€”â€Šit could peek inside directories and files, but not make anyÂ changes.</p><p>Now things are about to get both <strong>dangerous and fun</strong>. ğŸš€<br> Weâ€™re giving our agent the ability to <strong>write new files</strong> and even <strong>overwrite existingÂ ones</strong>.</p><p>This step marks a big shift: instead of just observing, our agent can now <strong>create and modify</strong>â€Šâ€”â€Ša true step toward acting like a real assistant.</p><h3>Assignment</h3><ul><li>Create a new function in your functions directory. Here&#39;s the signature IÂ used:</li></ul><pre>def write_file(working_directory, file_path, content):</pre><ul><li>If the file_path is outside of the working_directory, return a string with anÂ error:</li></ul><pre>f&#39;Error: Cannot write to &quot;{file_path}&quot; as it is outside the permitted working directory&#39;</pre><ul><li>If the file_path doesn&#39;t exist, create it. As always, if there are errors, return a string representing the error, prefixed with &quot;Error:&quot;.</li><li>Overwrite the contents of the file with the content argument.</li><li>If successful, return a string with theÂ message:</li></ul><pre>f&#39;Successfully wrote to &quot;{file_path}&quot; ({len(content)} characters written)&#39;</pre><p>Itâ€™s important to return a success string so that our LLM knows that the action it took actually worked. Feedback loops, feedback loops, feedbackÂ loops!</p><ul><li>Remove your old tests from tests.py and add three new ones, as always print the results ofÂ each:</li><li>write_file(&quot;calculator&quot;, &quot;lorem.txt&quot;, &quot;wait, this isn&#39;t loremÂ ipsum&quot;)</li><li>write_file(&quot;calculator&quot;, &quot;pkg/morelorem.txt&quot;, &quot;lorem ipsum dolor sitÂ amet&quot;)</li><li>write_file(&quot;calculator&quot;, &quot;/tmp/temp.txt&quot;, &quot;this should not be allowed&quot;)</li></ul><h3>Tips</h3><ul><li><a href=\"https://docs.python.org/3/library/os.path.html#os.path.exists\">os.path.exists</a>: Check if a pathÂ exists</li><li><a href=\"https://docs.python.org/3/library/os.html#os.makedirs\">os.makedirs</a>: Create a directory and all itsÂ parents</li><li><a href=\"https://docs.python.org/3/library/os.path.html#os.path.dirname\">os.path.dirname</a>: Return the directory name</li></ul><p>Example of writing to aÂ file:</p><pre>with open(file_path, &quot;w&quot;) as f:<br>    f.write(content)</pre><h3>5ï¸âƒ£ RunÂ Python</h3><p>If you thought allowing an LLM to write files was a badÂ ideaâ€¦</p><p>You ainâ€™t seen nothinâ€™ yet! (praise the <a href=\"https://en.wikipedia.org/wiki/Roko%27s_basilisk\">basilisk</a>)</p><p>Itâ€™s time to build the functionality for our Agent to <em>run arbitrary PythonÂ code</em>.</p><p>Now, itâ€™s worth pausing to point out the inherent security risks here. We have a few things going forÂ us:</p><ol><li>Weâ€™ll only allow the LLM to run code in a specific directory (the working_directory).</li><li>Weâ€™ll use a 30-second timeout to prevent it from running indefinitely.</li></ol><p>But aside from thatâ€¦ yes, the LLM can run arbitrary code that we (or it) places in the working directoryâ€¦ so be careful. As long as you only use this AI Agent for the simple tasks weâ€™re doing in this course you should be justÂ fine.</p><p>Do <strong>not</strong> give this program to others for them to use! It does not have all the security and safety features that a production AI agent would have. It is for learning purposesÂ only.</p><h3>Assignment</h3><ul><li>Create a new function in your functions directory called run_python_file. Hereâ€™s the signature toÂ use:</li></ul><pre>def run_python_file(working_directory, file_path, args=[]):</pre><ul><li>If the file_path is outside the working directory, return a string with anÂ error:</li></ul><pre>f&#39;Error: Cannot execute &quot;{file_path}&quot; as it is outside the permitted working directory&#39;</pre><ul><li>If the file_path doesnâ€™t exist, return an errorÂ string:</li></ul><pre>f&#39;Error: File &quot;{file_path}&quot; not found.&#39;</pre><ul><li>If the file doesnâ€™t <a href=\"https://docs.python.org/3/library/stdtypes.html#str.endswith\">end with</a> â€œ.pyâ€, return an errorÂ string:</li></ul><pre>f&#39;Error: &quot;{file_path}&quot; is not a Python file.&#39;</pre><ul><li>Use the <a href=\"https://docs.python.org/3/library/subprocess.html#subprocess.run\">subprocess.run</a> function to execute the Python file and get back a &quot;completed_process&quot; object. Make sureÂ to:</li><li>Set a timeout of 30 seconds to prevent infinite execution</li><li>Capture both stdout andÂ stderr</li><li>Set the working directory properly</li><li>Pass along the additional args ifÂ provided</li><li>Return a string with the output formatted toÂ include:</li><li>The stdout prefixed with STDOUT:, and stderr prefixed with STDERR:. The &quot;completed_process&quot; object has a stdout and stderr attribute.</li><li>If the process exits with a non-zero code, include â€œProcess exited with codeÂ Xâ€</li><li>If no output is produced, return â€œNo output produced.â€</li><li>If any exceptions occur during execution, catch them and return an errorÂ string:</li></ul><pre>f&quot;Error: executing Python file: {e}&quot;</pre><ul><li>Update your tests.py file with these test cases, printing eachÂ result:</li><li>run_python_file(&quot;calculator&quot;, &quot;main.py&quot;) (should print the calculator&#39;s usage instructions)</li><li>run_python_file(&quot;calculator&quot;, &quot;main.py&quot;, [&quot;3 + 5&quot;]) (should run the calculator... which gives a kinda nasty renderedÂ result)</li><li>run_python_file(&quot;calculator&quot;, &quot;tests.py&quot;)</li><li>run_python_file(&quot;calculator&quot;, &quot;../main.py&quot;) (this should return anÂ error)</li><li>run_python_file(&quot;calculator&quot;, &quot;nonexistent.py&quot;) (this should return anÂ error)</li></ul><h3>ğŸ¯ Wrapping Up ChapterÂ 2</h3><p>In this chapter, we leveled up our agent by giving it a real set ofÂ <strong>skills</strong>:</p><ul><li>ğŸ§® Work with our <strong>calculator project</strong></li><li>ğŸ“‚ <strong>List directories</strong> and inspect fileÂ metadata</li><li>ğŸ“„ <strong>Read file contents</strong>Â safely</li><li>ğŸ—‚ï¸ <strong>Fetch specificÂ files</strong></li><li>âœï¸ <strong>Write and overwrite files</strong></li><li>ğŸ <strong>Run Python code</strong> inside theÂ project</li></ul><p>With these abilities, our agent has moved from being a simple observer to an active participantâ€Šâ€”â€Šcapable of exploring, editing, and executing code in its environment.</p><p>But weâ€™re not stopping here. ğŸš€<br> In <strong>Chapter 3</strong>, weâ€™ll bring these functions together and <strong>integrate them with the LLM</strong>, so our agent can decide when and how to use them. Thatâ€™s when the magic really starts.Â âœ¨</p><p>ğŸ‘‰ Stay tuned for <strong>Chapter 3: Turning Functions into anÂ Agent</strong>.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3a789758d223\" width=\"1\" height=\"1\" alt=\"\">",
      "url": "https://rajdeep01.medium.com/build-an-ai-agent-chapter-2-functions-3a789758d223?source=rss-289e64b56868------2",
      "publishedAt": "Fri, 05 Sep 2025 20:56:04 GMT",
      "updatedAt": "Fri, 05 Sep 2025 20:56:04 GMT",
      "tags": [
        "llm",
        "chatgpt",
        "generative-ai-tools",
        "artificial-intelligence",
        "ai-agent"
      ],
      "author": "Rajdeep Singh",
      "image": "https://cdn-images-1.medium.com/max/875/0*IEyT6d8qG870TjeP"
    },
    {
      "id": "https://medium.com/p/8a06a0d236ef",
      "title": "Build an AI Agentâ€Šâ€”â€ŠChapter 1Â : LLMs",
      "description": "",
      "content": "<h3>Build an AI Agentâ€Šâ€”â€ŠChapter 1Â :Â LLMs</h3><p>If youâ€™ve ever played around with <strong>Cursor</strong> or <strong>Claude Code</strong> as an â€œagenticâ€ AI editor, you already have a good idea of what this project isÂ about.</p><p>Weâ€™re creating a <strong>mini version of Claude Code</strong>â€Šâ€”â€Šbut hereâ€™s the twist: weâ€™ll be building it with <strong>Googleâ€™s free Gemini API</strong>. The best part? As long as you have access to an LLM, itâ€™s surprisingly straightforward to put together a custom agent thatâ€™s not just fun, but also quite effective.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*4ybDsBqKElm-Hs-U\" /><figcaption>AI Agent</figcaption></figure><h3>ğŸ§‘â€ğŸ’»1. What Does This AgentÂ Do?</h3><p>Before we dive in, hereâ€™s what youâ€™ll need to getÂ started:</p><ul><li><strong>Python 3.10+</strong> installed</li><li><strong>uv project</strong> and packageÂ manager</li><li>Access to a <strong>Unix-like shell</strong> (zsh or bash worksÂ fine)</li></ul><h3>ğŸ¯ LearningÂ Goals</h3><p>This project is designed with a few key learning outcomes inÂ mind:</p><ul><li>Get comfortable working with <strong>multi-directory PythonÂ projects</strong></li><li>Understand how the AI tools youâ€™ll likely use in your future jobs <strong>actually work under theÂ hood</strong></li><li>Sharpen your <strong>Python and functional programming</strong> skills</li></ul><p>To be clear, the goal here isnâ€™t to train your own LLM from scratch. Instead, weâ€™ll be taking a <strong>pre-trained LLM</strong> and building an <strong>agent from the ground up</strong>â€Šâ€”â€Ša hands-on way to learn byÂ doing.</p><h3>2. PythonÂ Setup</h3><p>Letâ€™s set up a virtual environment for ourÂ project.</p><p>Virtual environments are Pythonâ€™s way to keep dependencies (e.g. the Google AI libraries weâ€™re going to use) separate from other projects on ourÂ machine.</p><h3>Assignment</h3><ul><li>Use uv to create a new project. It will create the directory and also initialize git.</li></ul><pre>uv init your-project-name<br>cd your-project-name</pre><ul><li>Create a virtual environment at the top level of your project directory:</li></ul><pre>uv venv</pre><p>Always add the venv directory to yourÂ .gitignore file.</p><ul><li>Activate the virtual environment:</li></ul><pre>source .venv/bin/activate</pre><p>You should see (your-project-name) at the beginning of your terminalÂ prompt.</p><p>Always make sure that your virtual environment is activated when running the code or using the Boot.devÂ CLI.</p><ul><li>Use uv to add two dependencies to the project. They will be added to the file pyproject.toml:</li></ul><pre>uv add google-genai==1.12.1<br>uv add python-dotenv==1.1.0</pre><p>This tells Python that this project requires <a href=\"https://pypi.org/project/google-genai/\">google-genai</a> version 1.12.1 and the <a href=\"https://pypi.org/project/python-dotenv/\">python-dotenv</a> versionÂ 1.1.0.</p><p>To run the project using the uv virtual environment, youÂ use:</p><pre>uv run main.py</pre><p>In your terminal, you should see Hello from YOUR PROJECTÂ NAME</p><h3>3. Gemini</h3><p><a href=\"https://www.cloudflare.com/learning/ai/what-is-large-language-model/\">Large Language Models (LLMs)</a> are the fancy-schmancy AI technology that have been making all the waves in the AI world recently. ProductsÂ like:</p><ul><li>ChatGPT</li><li>Claude</li><li>Cursor</li><li>Google Gemini</li></ul><p>â€¦ are all powered by LLMs. For the purposes of this project, you can think of an LLM as a smart text generator. It works just like ChatGPT: <em>you give it a prompt, and it gives you back some text that it believes answers your prompt</em>. Weâ€™re going to use <a href=\"https://ai.google.dev/gemini-api/docs/pricing\">Googleâ€™s Gemini API</a> to power our agent in this project. Itâ€™s reasonably smart, but more importantly for us, <em>it has a freeÂ tier</em>.</p><h3>Tokens</h3><p>You can think of tokens as the currency of LLMs. They are the way that LLMs measure how much text they have to process. Tokens are <a href=\"https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\"><em>roughly</em> 4 characters</a> for most models. Itâ€™s important when working with LLM APIs to understand how many tokens youâ€™reÂ using.</p><p>Weâ€™ll be staying well within the free tier limits of the Gemini API, but weâ€™ll still monitor our tokenÂ usage!</p><p>Be aware that all API calls, including those made during local testing, consume tokens from your free tier quota. If you exhaust your quota, you may need to wait for it to reset (typically 24 hours) to continue the lesson. Regenerating your API key will not reset yourÂ quota.</p><h3>Assignment</h3><ul><li>Create an account on <a href=\"https://aistudio.google.com/\">Google AI Studio</a> if you donâ€™t already haveÂ one</li><li>Click the â€œCreate API Keyâ€Â button.</li></ul><p>If you already have a GCP account and a project, you can create the API key in that project. If you donâ€™t, AI studio will automatically create one forÂ you.</p><ul><li>Copy the API key, then paste it into a newÂ .env file in your project directory. The file should look likeÂ this:</li></ul><pre>GEMINI_API_KEY=&quot;your_api_key_here&quot;</pre><ul><li>Add theÂ .env file to yourÂ .gitignore</li></ul><p>We never want to commit API keys, passwords, or other sensitive information toÂ git.</p><ul><li>Update the main.py file. When the program starts, load the environment variables from theÂ .env file using the dotenv library and read the APIÂ key:</li></ul><pre>import os<br>from dotenv import load_dotenv</pre><pre>load_dotenv()<br>api_key = os.environ.get(&quot;GEMINI_API_KEY&quot;)</pre><ul><li>Import the genai library and use the API key to create a new instance of a <a href=\"https://googleapis.github.io/python-genai/#create-a-client\">GeminiÂ client</a>:</li></ul><pre>from google import genai</pre><pre>client = genai.Client(api_key=api_key)</pre><ul><li>Use the <a href=\"https://googleapis.github.io/python-genai/#generate-content\">client.models.generate_content() method</a> to get a response from the <a href=\"https://ai.google.dev/gemini-api/docs/models\">gemini-2.0-flash-001 model</a>! You&#39;ll need to use two <em>named</em> parameters:</li><li>model: The model name: gemini-2.0-flash-001 (this one has a generous freeÂ tier)</li><li>contents: The prompt to send to the model (a string). Use thisÂ prompt:</li></ul><p>The generate_content method returns a <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentResponse\">GenerateContentResponse object</a>. Print theÂ <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentResponse.text\">.text property</a> of the response to see the model&#39;sÂ answer.</p><p>If everything is working as intended, you should be able to run your code and see the modelâ€™s response in your terminal!</p><ul><li>In addition to printing the text response, print the number of tokens consumed by the interaction in thisÂ format:</li></ul><pre>Prompt tokens: X<br>Response tokens: Y</pre><p>The response has aÂ <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentResponseDict.usage_metadata\">.usage_metadata</a> property that hasÂ both:</p><ul><li>a prompt_token_count property (tokens in theÂ prompt)</li><li>a candidates_token_count property (tokens in the response)</li></ul><p><strong>Important Note: </strong>The Gemini API is an <em>external web service</em> and on occasion itâ€™s <em>slow and unreliable</em>. Itâ€™s possible in this course for you to lose armor because of an API outage on Googleâ€™s endâ€¦ just be sure to always run before submitting to minimize the risk of that happening.</p><h3>5. Input</h3><p>Weâ€™ve hardcoded the prompt that goes to gemini, which isâ€¦ not very useful. Letâ€™s update our code to accept the prompt as a command line argument.</p><p>We donâ€™t want our users to have to edit the code to change theÂ prompt!</p><h3>Assignment</h3><ul><li>Update your code to accept a command line argument for the prompt. ForÂ example:</li></ul><pre>uv run main.py &quot;Why are episodes 7-9 so much worse than 1-6?&quot;</pre><p>The <a href=\"https://docs.python.org/3/library/sys.html#sys.argv\">sys.argv</a> variable is a list of strings representing all the command line arguments passed to the script. The first element is the name of the script, and the rest are the arguments. Be sure to import sys to useÂ it.</p><ul><li>If the prompt is not provided, print an error message and exit the program with exit codeÂ 1.</li></ul><h3>5. Messages</h3><p>LLM APIs arenâ€™t typically used in a â€œone-shotâ€ manner, forÂ example:</p><ul><li>Prompt: â€œWhat is the meaning ofÂ life?â€</li><li>Response: â€œ42â€</li></ul><p>They work the same way ChatGPT works: <em>in a conversation</em>. The conversation has a history, and if we keep track of that history, then with each new prompt, the model can see the entire conversation and respond <em>within the larger context of the conversation</em>.</p><h3>Roles</h3><p>Importantly, each message in the conversation has a â€œroleâ€. In the context of a chat app like ChatGPT, your conversations would look likeÂ this:</p><ul><li><strong>user</strong>: â€œWhat is the meaning ofÂ life?â€</li><li><strong>model</strong>: â€œ42â€</li><li><strong>user</strong>: â€œWait, what did you justÂ say?â€</li><li><strong>model</strong>: â€œ42. Itâ€™s is the answer to the ultimate question of life, the universe, and everything.â€</li><li><strong>user</strong>: â€œButÂ why?â€</li><li><strong>model</strong>: â€œBecause Douglas Adams saidÂ so.â€</li></ul><p>So, while our program will still be â€œone-shotâ€ <em>for now</em>, letâ€™s update our code to store a list of messages in the conversation, and pass in the â€œroleâ€ appropriately.</p><h3>Assignment</h3><ul><li>Create a new list of <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.types.Content\">types.Content</a>, and set the user&#39;s prompt as the only message (forÂ now):</li></ul><pre>from google.genai import types</pre><pre>messages = [<br>    types.Content(role=&quot;user&quot;, parts=[types.Part(text=user_prompt)]),<br>]</pre><ul><li>Update your call to <a href=\"https://googleapis.github.io/python-genai/genai.html#genai.models.Models.generate_content\">models.generate_content</a> to use the messagesÂ list:</li></ul><pre>response = client.models.generate_content(<br>    model=&quot;gemini-2.0-flash-001&quot;,<br>    contents=messages,<br>)</pre><p>In the future, weâ€™ll add more messages to the list as the agent does its tasks in aÂ loop.</p><h3>6. Verbose</h3><p>As you debug and build your AI agent, youâ€™ll probably want to dump a lot more context into the consoleâ€¦ but at the same time, we donâ€™t want to make the user experience of our CLI tool tooÂ noisyâ€¦</p><p>Letâ€™s add an optional command line flag, --verbose, that will allow us to toggle &quot;verbose&quot; output on and off. When we want to see more info, we&#39;ll just turn that bad boyÂ on.</p><h3>Assignment</h3><ul><li>Add a new command line argument, --verbose. It should be supplied <em>after</em> the prompt if included. ForÂ example:</li></ul><pre>uv run main.py &quot;What is the meaning of life?&quot; --verbose</pre><ul><li>If the --verbose flag is included, the console output shouldÂ include:</li><li>The userâ€™s prompt: &quot;User prompt: {user_prompt}&quot;</li><li>The number of prompt tokens on each iteration: &quot;Prompt tokens: {prompt_tokens}&quot;</li><li>The number of response tokens on each iteration: &quot;Response tokens: {response_tokens}&quot;</li></ul><p>Otherwise, it should <em>not</em> print thoseÂ things.</p><h3>ğŸš€ Wrapping Up ChapterÂ 1</h3><p>In this chapter, we explored the fundamentalsâ€Šâ€”â€Šsetting up Gemini with Python, understanding token usage, experimenting with verbose messages, and handling input effectively. These steps might feel small, but together they form the backbone of every agent youâ€™llÂ build.</p><p>Now that the groundwork is done, itâ€™s time to bring our agent toÂ life.</p><p>ğŸ‘‰ In <strong>Chapter 2</strong>, weâ€™ll dive into the exciting part: <strong>writing the core logic that makes Gemini act like an agent</strong>â€Šâ€”â€Šprocessing instructions, responding intelligently, and feeling less like a script and more like a teammate.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8a06a0d236ef\" width=\"1\" height=\"1\" alt=\"\">",
      "url": "https://rajdeep01.medium.com/build-an-ai-agent-chapter-1-llms-8a06a0d236ef?source=rss-289e64b56868------2",
      "publishedAt": "Fri, 05 Sep 2025 14:55:00 GMT",
      "updatedAt": "Fri, 05 Sep 2025 14:55:00 GMT",
      "tags": [
        "artificial-intelligence",
        "aws",
        "google",
        "ai-agent",
        "llm"
      ],
      "author": "Rajdeep Singh",
      "image": "https://cdn-images-1.medium.com/max/1024/0*4ybDsBqKElm-Hs-U"
    },
    {
      "id": "https://medium.com/p/8cb9861e89dd",
      "title": "AI Trends 2025: Beyond the Hype, Towards Practical Reality",
      "description": "",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*wXV3uSaKqqJMWvSj.jpg\" /></figure><p>Artificial Intelligence has always been wrapped in a mix of excitement, speculation, and bold promises. In 2022, when ChatGPT-3.5 stunned the world, the AI hype reached its peak. By 2025, however, the conversation looks very different. The landscape has matured, the hype has cooled, and companies are shifting focus from â€œmagical breakthroughsâ€ to <strong>real-world ROI, cost optimization, and domain-specific models</strong>.</p><p>So, what are the biggest trends shaping AI in 2025 and beyond? Letâ€™s break themÂ down.</p><h3>1. Diminishing Returns from Scaling LargeÂ Models</h3><p>The era of simply making models bigger is showing cracks. From GPT-3.5 to GPT-5.2, each jump has delivered <strong>smaller and smaller improvements relative to the investment required</strong>.</p><p>Itâ€™s not that models arenâ€™t improvingâ€Šâ€”â€Šthey areâ€Šâ€”â€Šbut the exponential â€œwow factorâ€ of earlier generations is gone. Across OpenAI, Google, Meta, and others, scaling has hit a point of diminishing returns.</p><p><strong>Takeaway:</strong> Bigger isnâ€™t always better. Efficiency and smarter architectures will define the nextÂ wave.</p><h3>2. Cost Takes CenterÂ Stage</h3><p>In 2025, companies donâ€™t just ask, <em>â€œHow smart is the AI?â€</em> They ask, <em>â€œHow much does it cost toÂ run?â€</em></p><p>Running massive LLMs is expensive. Thatâ€™s why businesses increasingly rely on <strong>smaller, fine-tuned models</strong> that can be optimized for domain-specific tasks. Even medium-sized organizations now train their own models, cutting costs without sacrificing performance.</p><p><strong>Takeaway:</strong> Cost-effectiveness beats raw intelligence. The AI arms race is shifting from â€œscale at all costsâ€ to â€œmake it affordable andÂ usable.â€</p><h3>3. Rise of Company-Specific Models</h3><p>From NASA predicting wildfires and glacier movement, to Netflix refining its recommendation engine, companies are building <strong>their own foundation models</strong> trained on proprietary data.</p><p>Why?</p><ul><li>Theyâ€™re cheaper toÂ run.</li><li>They offer fullÂ control.</li><li>The data quality isÂ better.</li></ul><p>This marks a turning point: instead of depending solely on general-purpose LLMs, organizations are <strong>owning their AI pipelines</strong> and tailoring them to their businessÂ needs.</p><p><strong>Takeaway:</strong> Expect every major company to have its own AI model byÂ 2030.</p><h3>4. Market Forces Are Reshaping AI</h3><p>The hype around LLMs has fallen dramatically since 2022. Back then, people thought AGI was just around the corner. By 2025, reality has setÂ in:</p><ul><li>LLMs hallucinate.</li><li>They canâ€™t set their ownÂ goals.</li><li>They lack true logical consistency.</li></ul><p>Add to this a <strong>shortage of high-quality training data</strong>, and itâ€™s clear why progress feels slower. What weâ€™ll likely see instead is a push for <strong>new architectures</strong>â€Šâ€”â€Šlike Yann LeCunâ€™s <strong>Joint Embedding Predictive Architecture (JEPA)</strong>â€Šâ€”â€Šthat promise better reasoning and internal consistency.</p><h3>5. The Future: Practical AI, NotÂ Sci-Fi</h3><p>Looking ahead to 2030 and beyond, hereâ€™s where the momentum isÂ headed:</p><ul><li><strong>More engineers hired</strong> to integrate AI into real-world systems.</li><li><strong>New architectures</strong> that move past todayâ€™s LLM limitations.</li><li><strong>More honesty in marketing</strong>â€Šâ€”â€Šcompanies will stop promising AGI â€œnext yearâ€ and focus on cost savings, integration, and domain-specific useÂ cases.</li></ul><p>AI isnâ€™t replacing humans anytime soonâ€Šâ€”â€Šbut it is becoming a powerful co-pilot, embedded into every aspect of business.</p><h3>Final Thoughts</h3><p>AI in 2025 isnâ€™t about hypeâ€Šâ€”â€Šitâ€™s about <strong>practical adoption</strong>. The magic is fading, but whatâ€™s replacing it is more important: mature, reliable, and cost-efficient AI systems that businesses can actuallyÂ trust.</p><p>As Geoffrey Hinton and Yann LeCun remind us, progress in AI is real but requires patience and honesty. The future wonâ€™t be built on exaggerated claims, but on steady innovation and thoughtful application.</p><p><strong>The bottom line:</strong> The next decade of AI wonâ€™t be about chasing AGI dreamsâ€Šâ€”â€Šit will be about building AI that works, scales, and delivers realÂ value.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8cb9861e89dd\" width=\"1\" height=\"1\" alt=\"\">",
      "url": "https://rajdeep01.medium.com/ai-trends-2025-beyond-the-hype-towards-practical-reality-8cb9861e89dd?source=rss-289e64b56868------2",
      "publishedAt": "Sun, 31 Aug 2025 09:01:10 GMT",
      "updatedAt": "Sun, 31 Aug 2025 09:01:10 GMT",
      "tags": [
        "chatgpt",
        "large-language-models",
        "ai",
        "llm",
        "artificial-intelligence"
      ],
      "author": "Rajdeep Singh",
      "image": "https://cdn-images-1.medium.com/max/1024/0*wXV3uSaKqqJMWvSj.jpg"
    },
    {
      "id": "https://medium.com/p/2e692f7703c1",
      "title": "Library vs Framework: Whoâ€™s Really in Control?",
      "description": "",
      "content": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/926/1*zvClG6AbVrPsFoicoRDGpw.png\" /></figure><p>Ever felt like your code is bossing you around? Or maybe youâ€™ve had days where youâ€™re the boss, commanding little pieces of code to do your bidding. The truth isâ€Šâ€”â€Šthat dynamic depends on whether youâ€™re using a <strong>library</strong> or a <strong>framework</strong>. Letâ€™s break this down in plain English (with a dash of humor and some code along theÂ way).</p><h3>ğŸš– The Taxi vs The BusÂ Analogy</h3><ul><li><strong>Library â†’ Taxi</strong><br> You call it when you need it, tell it where to go, and it takes you there. <strong>Youâ€™re inÂ control.</strong></li><li><strong>Framework â†’ Bus</strong><br> It has fixed stops, fixed routes, and you step in and follow its rules. The <strong>framework is in control</strong>, and youâ€™re just a passenger (though it saves you from the stress of driving).</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*9LOFXiEBTJkc6086\" /></figure><h3>âš™ï¸ Technical Difference</h3><h3>1. Library</h3><ul><li>A <strong>collection of reusable tools</strong> you call whenÂ needed.</li><li><strong>You decide</strong> how and when to useÂ them.</li><li><strong>Control flow stays withÂ you.</strong></li></ul><p>Example (C++Â STL):</p><pre>#include &lt;algorithm&gt;<br>#include &lt;vector&gt;<br>int main() {<br>    std::vector&lt;int&gt; nums = {3, 1, 2};<br>    std::sort(nums.begin(), nums.end()); // You call it when you want<br>}</pre><p>Here, youâ€™re in chargeâ€Šâ€”â€Šyou decide when sortÂ runs.</p><h3>2. Framework</h3><ul><li>Provides a <strong>structure and rules</strong> for your application.</li><li><strong>Framework decides</strong> when your codeÂ runs.</li><li>Known as <strong>Inversion of Control</strong> â†’ instead of you calling the framework, the framework callsÂ you.</li></ul><p>Example (Django):</p><pre># Django example<br>from django.http import HttpResponse</pre><pre>def my_view(request):<br>    # You didnâ€™t call this â€” Django did!<br>    return HttpResponse(&quot;Hello, Framework!&quot;)</pre><p>You never directly call my_view. Django calls it for you when a user visits a specificÂ route.</p><h3>ğŸ¬ Netflix vs TVÂ Schedule</h3><ul><li><strong>Library = Netflix</strong><br> You pick the show, pause it, binge it at 3 AM. TotalÂ freedom.</li><li><strong>Framework = TV Schedule</strong><br> The show airs at 8 PM. Donâ€™t like it? Too badâ€Šâ€”â€Šthe framework decides theÂ flow.</li></ul><h3>ğŸ“Š Infographic (MentalÂ Image)</h3><p><strong>Whoâ€™s Driving theÂ Code?</strong></p><p>Library â†’ You â†’ Library â†’ Output<br> Framework â†’ Framework â†’ Your Code â†’Â Output</p><p>Type Examples Whoâ€™s in Control? Pros Cons Library STL, Lodash, NumPy Developer Flexible, reusable Can cause chaos in big teams Framework Django, Angular, Rails Framework Structured, consistent Less freedom, steep learningÂ curve</p><h3>ğŸ¯ Why ItÂ Matters</h3><ul><li><strong>Flexibility</strong>: Libraries give you freedom to mix and match. Great for prototyping.</li><li><strong>Consistency</strong>: Frameworks enforce patterns. Great for scalingÂ teams.</li><li><strong>Architecture Choices</strong>: Choosing between the two shapes how easy (or painful) your project will be to maintain.</li></ul><h3>ğŸ¤” Soâ€¦ Which Should YouÂ Use?</h3><ul><li>If youâ€™re an indie hacker or building small projects â†’ <strong>Library</strong> gives you speed andÂ control.</li><li>If youâ€™re working with a team on a large project â†’ <strong>Framework</strong> saves you from spaghetti code.</li></ul><h3>ğŸ’¡ FinalÂ Thought</h3><p>With a library, <strong>you hold the steering wheel</strong>. With a framework, <strong>you sit in the passenger seat</strong>â€Šâ€”â€Šbut hey, at least someone else is dealing with theÂ traffic.</p><p>ğŸ‘¨â€ğŸ’» Your turn: Which do you preferâ€Šâ€”â€Šthe taxi freedom of libraries, or the bus structure of frameworks? Drop a comment below and letâ€™sÂ debate!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2e692f7703c1\" width=\"1\" height=\"1\" alt=\"\">",
      "url": "https://rajdeep01.medium.com/library-vs-framework-whos-really-in-control-2e692f7703c1?source=rss-289e64b56868------2",
      "publishedAt": "Fri, 29 Aug 2025 19:01:11 GMT",
      "updatedAt": "Fri, 29 Aug 2025 19:01:11 GMT",
      "tags": [
        "react",
        "javascript",
        "nextjs",
        "framework",
        "libraries"
      ],
      "author": "Rajdeep Singh",
      "image": "https://cdn-images-1.medium.com/max/926/1*zvClG6AbVrPsFoicoRDGpw.png"
    }
  ]
}